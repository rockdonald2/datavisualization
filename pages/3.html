<!DOCTYPE html>
<html lang="en"
    class="text-gray-800 text-base md:text-lg antialiased leading-relaxed tracking-normal break-normal bg-white">

    <head>
        <meta charset="UTF-8">
        <meta name="viewport"
            content="width=device-width, initial-scale=1.0">
        <meta name="description"
            content="A repo for studying a book.">
        <meta name="keywords"
            content="visualization,javascript,python,d3,flask,restful">
        <meta name="author"
            content="Luk√°cs Zsolt">
        <meta http-equiv="X-UA-Compatible"
            content="ie=edge">
        <title>3 Az adatok megszerz√©se</title>
        <link rel="stylesheet"
            href="../css/style.css">
        <link rel="stylesheet"
            href="../css/compiled.css">
        <link rel="stylesheet"
            href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/monokai-sublime.min.css">
        <link rel="stylesheet"
            href="../tools/pushy-1.3.0/css/pushy.css">
        <!-- Safari-n az emoji favicon nem m≈±k√∂dik -->
        <link rel="icon"
            href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22
                viewBox=%220
                0
                100
                100%22><text y=%22.9em%22
                    font-size=%2290%22>üìï</text></svg>">
    </head>

    <body class="overflow-x-hidden">
        <!-- navig√°ci√≥ -->
        <nav class="pushy pushy-left">
            <div class="pushy-content">
                <ul>
                    <li class="pushy-link"><a href="../index.html">1 Bevezet√©s</a></li>
                    <li class="pushy-link"><a href="2.html">2 Els≈ë l√©p√©sek az adatvizualiz√°ci√≥ban</a></li>
                    <li class="pushy-link"><a href="3.html">3 Az adatok megszerz√©se</a></li>
                </ul>
            </div>
        </nav>

        <!-- Pushy-hoz site overlay -->
        <div class="site-overlay"></div>

        <main id="container"
            class="container p-8 md:p-6 max-w-2xl md:max-w-3xl min-h-screen">
            <button id="menu"
                class="menu-btn px-4 py-2 bg-gray-900 text-white hover:bg-blue-800 transition duration-75 ease-in">&#9776;
                Men√º</button>

            <!-- harmadik r√©sze eleje -->

            <section class="pb-8">
                <h1 class="text-3xl md:text-4xl py-8 text-gray-900 font-semibold">3 Az adatok megszerz√©se</h1>

                <section id="3python">
                    <h2 class="text-2xl pb-4 font-semibold text-gray-800">Szerezz√ºk meg az adatot a netr≈ël Python-nal</h2>
                    <p id="firstPar" class="text-lg font-normal pb-3">A k√∂nyv ennek a r√©sz√©ben nekikezd√ºnk a val√≥di adat-vizualiz√°ci√≥s
                        folyamatnak
                        az
                        adatokkal megszerz√©s√©vel.</p>
                    <p class="text-lg font-normal pb-3">Alapvet≈ëen elmondhat√≥, hogy n√©lk√ºl√∂zhetetlen r√©sze az
                        adat-megjelen√≠t√©snek
                        maga
                        az
                        adat, ami nem minden esetben √°ll k√©szen sz√°munkra. A lehet≈ë legjobb √©s legtiszt√°bb m√≥don kell azt
                        megszerezn√ºnk √©s ebben nagy seg√≠ts√©g√ºnkre van a Python. Sz√°mos Python k√∂nyvt√°r √°ll rendelkez√©s√ºnkre a
                        folyamat
                        v√©grehajt√°s√°ra. A legf≈ëbb m√≥dok arra ahogyan adatot tudunk let√∂lteni a netr≈ël: HTTP-n kereszt√ºl
                        megszereezz√ºk
                        a nyers adatf√°jlt, valamilyen ismert form√°tumban, legyen az CSV vagy JSON; ak√°r egy erre dedik√°lt API-t
                        is
                        haszn√°lhatunk; megszerezz√ºk az adatot az √°ltal, hogy let√∂ltj√ºk a weboldalt HTTP-n kereszt√ºl √©s
                        √©rtelmezz√ºk
                        helyileg a sz√ºks√©ges adatok√©rt.</p>
                    <p class="text-lg font-normal">El≈ësz√∂ris, n√©zz√ºk meg az egyik legelterjedtebb Python HTTP k√∂nyvt√°rt a
                        requests-et. Ahogyan megbesz√©lt√ºk a k√∂nyv el≈ëz≈ë r√©sz√©ben azon f√°jlok, amelyek a b√∂ng√©sz≈ëben megjelennek
                        val√≥j√°ban HTTP-n kereszt√ºl ker√ºlnek tov√°bb√≠t√°sra. Teh√°t, a webes tartalmat el≈ësz√∂r lekell k√©rni egy GET
                        request √°ltal, miel≈ëtt azt √©rtelmezni lehessen. Mivel a requests nem r√©sze a Python szabv√°ny k√∂nyvt√°rnak
                        el≈ësz√∂r
                        le kell azt t√∂lts√ºk a k√∂vetkez≈ë paranccsal:</p>
                    <pre
                        class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="shell">$ pip install requests</code></pre>
                    <p class="text-lg font-normal">Miut√°n let√∂lt√∂tt√ºk neki is foghatunk a folyamatnak. Els≈ë l√©p√©sben pr√≥b√°ljuk a
                        k√∂nyvt√°r seg√≠ts√©g√©vel let√∂lteni egy Wikip√©dia oldalt. Haszn√°lhatjuk erre a requests k√∂nyvt√°r get
                        met√≥dus√°t,
                        amit a k√∂vetkez≈ë k√≥dr√©szlet is szeml√©ltet:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">import requests

response = requests.get('https://en.wikipedia.org/wiki/Nobel_Prize')

response # output: &#60;Response [200]>

dir(response) # output: a response objektum attrib√∫tumjai</code></pre>
                    <p class="text-lg font-normal">A fenti 200-as HTTP st√°tusz k√≥d l√©nyeg√©ben a HTTP megfelel≈ëje a 'minden
                        rendben
                        volt'-nak. Ebben a kontextusban ez azt jelenti, hogy bizonyosan valamilyen inform√°ci√≥t a GET request
                        visszat√©r√≠tett, a k√∂vetkez≈ë paranccsal ak√°r meg is vizsg√°lhatjuk mit:</p>
                    <pre
                        class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">response.headers</code></pre>
                    <p class="text-lg font-normal">N√©h√°ny nagyon √©rdekes dolgot tudunk megfigyelni a headers attrib√∫tummal,
                        p√©ld√°ul
                        azt, hogy a weboldalt tartalma <em>text/html</em>, a tartalom enk√≥dol√°sa <em>gzip</em>, stb:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">{'Date': 'Sun, 05 Jul 2020 15:09:55 GMT', 'Server': 'mw1407.eqiad.wmnet', 
'X-Content-Type-Options': 'nosniff', 'P3p': 'CP="See https://en.wikipedia.org/wiki/Special:CentralAutoLogin/P3P for more info."', 'Content-Language': 'en', 'Vary': 'Accept-Encoding,Cookie,Authorization', 'Last-Modified': 'Sun, 05 Jul 2020 14:58:26 GMT', 
'Content-Type': 'text/html; charset=UTF-8', 'Content-Encoding': 'gzip', 'Age': '10136', 'X-Cache': 'cp3054 miss, cp3064 hit/19', 'X-Cache-Status': 'hit-front', 'Server-Timing': 'cache;desc="hit-front"', 'Strict-Transport-Security': 'max-age=106384710; includeSubDomains; preload', 
'Set-Cookie': 'WMF-Last-Access=05-Jul-2020;Path=/;HttpOnly;secure;Expires=Thu, 06 Aug 2020 12:00:00 GMT, WMF-Last-Access-Global=05-Jul-2020;Path=/;Domain=.wikipedia.org;HttpOnly;secure;Expires=Thu, 06 Aug 2020 12:00:00 GMT, GeoIP=MD:CU:Chisinau:47.01:28.86:v4; Path=/; secure; Domain=.wikipedia.org', 
'X-Client-IP': '178.17.168.163', 'Cache-Control': 'private, s-maxage=0, max-age=0, must-revalidate', 
'Accept-Ranges': 'bytes', 'Content-Length': '81973', 'Connection': 'keep-alive'}</code></pre>
                    <p class="text-lg font-normal">Mivel, a fenti vizsg√°lattal megtudtuk azt, hogy az eredm√©ny egy sz√∂veg, ak√°r
                        le is
                        k√©rhetj√ºk a val√≥di tartalm√°t:</p>
                    <pre
                        class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">response.text</code></pre>
                    <p class="text-lg font-normal">A fenti k√≥dr√©szlet visszat√©r√≠ti a GET requests √°ltal lek√©rt teljes HTML
                        sz√∂veget.
                        √ñsszes√©g√©benn a requests egy el√©g k√©nyelmes megold√°s arra, hogy gyorsan szerezz√ºnk webes adatot a Python
                        programukba. P√©ldak√©ppen k√©rj√ºk le egy adatsettet az Eurostatr√≥l, kicsit neh√©zkes feladat megtal√°lni a
                        megfelel≈ë el√©r√©si √∫tat, de a k√∂vetkez≈ë p√©lda ezt szeml√©lteti:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">response = requests.get('http://ec.europa.eu/eurostat/wdds/rest/data/v2.1/json/en/nama_10_gdp?geo=EU28&precision=1&na_item=B1GQ&unit=CP_MEUR&time=2018&time=2019')

response # output: &#60;Response [200]></code></pre>
                    <p class="text-lg font-normal">Mivel a fenti esetben a kapott f√°jl egy JSON √°llom√°ny, a requests k√∂nyvt√°r
                        rendelkezik egy met√≥dussal, amely lehet≈ëv√© teszi azt, hogy el√©rj√ºk a v√°lasz adatj√°t, mint egy Python
                        dict. Ez
                        tartalmazni fogja a metaadatot √©s egy list√°nyi adatelemet:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">response.json()

""" 
output: 

{'version': '2.0',
 'label': 'GDP and main components (output, expenditure and income)',
 'href': 'http://ec.europa.eu/eurostat/wdds/rest/data/v2.1/json/en/nama_10_gdp?geo=EU28&precision=1&na_item=B1GQ&unit=CP_MEUR&time=2018&time=2019',
 'source': 'Eurostat',
 'updated': '2020-07-01',
 'extension': {'datasetId': 'nama_10_gdp',
  'lang': 'EN',
  'description': None,
  'subTitle': None},
 'class': 'dataset',
 'value': {'0': 15915732.9, '1': 16452065.5},
 'dimension': {'unit': {'label': 'unit',
   'category': {'index': {'CP_MEUR': 0},
    'label': {'CP_MEUR': 'Current prices, million euro'}}},
  'na_item': {'label': 'na_item',
   'category': {'index': {'B1GQ': 0},
    'label': {'B1GQ': 'Gross domestic product at market prices'}}},
  'geo': {'label': 'geo',
   'category': {'index': {'EU28': 0},
    'label': {'EU28': 'European Union - 28 countries (2013-2020)'}}},
  'time': {'label': 'time',
   'category': {'index': {'2018': 0, '2019': 1},
    'label': {'2018': '2018', '2019': '2019'}}}},
 'id': ['unit', 'na_item', 'geo', 'time'],
 'size': [1, 1, 1, 2]}
"""</code></pre>
                    <p class="text-lg font-normal pb-3">Az el≈ëz≈ë p√©ld√°kban mindig 'nyers' m√≥don weboldalakat, vagy JSON
                        √°llom√°nyokat
                        √©rt√ºnk
                        el a requests k√∂nyvt√°r seg√≠ts√©g√©vel, a k√∂vetkez≈ëkben n√©zz√ºk meg azt is mik√©nt tudjuk API-kkal val√≥
                        kommunik√°ci√≥kra is haszn√°lni. Az API-kkal val√≥ kommunik√°ci√≥ra akkor van sz√ºks√©g, amennyiben az adat amit
                        keres√ºnk nem tal√°lhat√≥ meg a web-en, de tal√°n el√©rhetj√ºk egy API-n kereszt√ºl. Ez mag√°ba foglalja azt,
                        hogy egy
                        lek√©r√©st int√©z√ºnk a megfelel≈ë szerverhez, hogy el√©rj√ºk az adatot egy fix vagy √°ltalunk v√°laszthat√≥
                        form√°tumban.</p>
                    <p class="text-lg font-normal pb-3">A web-es API-k √°ltal leggyakrabban haszn√°lt form√°tumok a JSON √©s az XML.
                        Sz√°munkra
                        az els≈ë lesz a fontos, kor√°bban eml√≠tett okokb√≥l kifoly√≥lag. Sz√°mos megk√∂zel√≠t√©s van arra, hogy arra,
                        hogy
                        l√©trehozzunk egy web-es API-t, √©s egy j√≥ ideig egy kis h√°bor√∫hoz hasonl√≠t√≥ harc volt a k√ºl√∂nb√∂z≈ë domin√°ns
                        architekt√∫r√°k k√∂z√∂tt: REST vs. XML-RPC vs. SOAP. Manaps√°g, a REST t≈±nik diadalmasnak, ami egy nagyon j√≥
                        dolog,
                        hiszen egyszer≈±bb a haszn√°lata √©s k√∂nnyedebb, mint b√°rmelyik alternat√≠va, szerencs√©re a szabv√°nyos√≠t√°snak
                        k√∂sz√∂nhet≈ëen nagyon val√≥sz√≠n≈±v√© teszi azt, hogy felismered √©s gyorsan √°tszoksz b√°rmely √∫j API-hoz, amivel
                        b√°rmikor is tal√°lkozol. Ide√°lis, ak√°r a k√≥dodat is teljesen √∫jrahaszn√°lhatod.</p>
                    <p class="text-lg font-normal pb-3">A legt√∂bb t√°voli adat el√©r√©s √©s manipul√°l√°s n√©gy m≈±veletben
                        √∂sszefoglalhat√≥:
                        hozzunk l√©tre valamit, k√©rj√ºnk le valamit, friss√≠ts√ºnk valamit √©s v√©g√ºl t√∂r√∂lj√ºnk valamit (CRUD). A HTTP
                        biztos√≠tja
                        ezen alapvet≈ë m≈±veleteket a POST, GET, PUT, DELETE utas√≠t√°sokkal, √©s a REST pontosan ezen utas√≠t√°sok
                        absztrakci√≥j√°ra √©p√ºl, mindenk√∂zben √©p√≠tve az URI-ra.</p>
                    <p class="text-lg font-normal pb-3">Sz√°mos vita van arr√≥l, hogy mi a megfelel≈ë √©s mi a nem megfelel≈ë RESTful
                        interf√©sz,
                        de l√©nyeg√©ben az URI-nak <em>(pl. http://example.com/api/items/2)</em>, tartalmaznia kellene mindazon
                        inform√°ci√≥kat
                        amelyek sz√ºks√©gesek ahhoz, hogy v√©gbemehessen egy eml√≠tett CRUD m≈±velet. √ögy lehet elk√©pzelni az eg√©sz
                        munk√°t,
                        hogy az URI a virtu√°lis el√©r√©se az adatnak, a CRUD pedig a m≈±veleteket jelenti, amelyeket az adattal
                        k√©pesek
                        vagyunk elv√©gezni.</p>
                    <p class="text-lg font-normal">N√©zz√ºk meg teh√°t hogyan tudjuk a requests k√∂nyvt√°rat RESTful web API el√©r√©s√©re
                        haszn√°lni. Ebben az esetben m√°r √∂sszetetebb folyamatokra haszn√°ljuk a k√∂nyvt√°r, argumentumokkal ell√°tott
                        URL-t
                        pr√≥b√°lunk el√©rni vele. Az al√°bbi k√≥dr√©szlet szeml√©lteti azt, hogy milyen f√ºggv√©nyt kellene √≠rni ahhoz,
                        hogy az
                        OECD adatb√°zisb√≥l az API-j√ºk√∂n kereszt√ºl √©rj√ºnk el adatokat:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">OECD_ROOT_URL = 'http://stats.oecd.org/sdmx-json/data'

def make_OECD_request(dsname, dimensions, params=None, root_dir=OECD_ROOT_URL):
	""" L√©trehozza az URL-t, majd visszat√©r√≠ti a GET request v√°lasz√°t """

	if not params:
		params = {}
	# az√©rt van a fentire sz√ºks√©g, mert nem szabad alap√©rtelmezett param√©terekk√©nt v√°ltoztathat√≥ Python √©rt√©keket megadni

	dim_args = ['+'.join(d) for d in dimensions]
	dim_str = '.'.join(dim_args)
	# a n√©gy dimenzi√≥s argumentumok list√°ja, argumentumokon bel√ºl +-al √∂sszef≈±zve, az argumentumok sor√°t pedig .-al √∂sszef≈±zve
	# pl. AUS+AUT.GDP+B1_G3.CUR+VOBARSA.Q

	url = root_dir + '/' + dsname + '/' + dim_str + '/all'

	print('Requesting OECD URL: {}'.format(url))

	return requests.get(url, params=params)
	# a requests GET met√≥dusa ak√°r egy param√©ter dict-et is k√©pes bevenni, hogy az URL-t tov√°bb alak√≠tsa, egy URL query string-g√©
	# pl. startTime=2009-Q2&endTime=2011-Q4

response = make_OECD_request('QNA', (('USA', 'AUS'), ('GDP', 'B1_GE'), ('CUR', 'VOBARSA'), ('Q')), {'startTime': '2009-Q1', 'endTime': '2010-Q1'})
# output: Requesting OECD URL: http://stats.oecd.org/sdmx-json/data/QNA/USA+AUS.GDP+B1_GE.CUR+VOBARSA.Q/all</code></pre>
                    <p class="text-lg font-normal">Miut√°n l√©trehoztuk a linket √©s a lek√©rdez√©s ut√°n megkaptuk r√° a v√°laszt is,
                        amely
                        tartalmazza a gazdas√°gi adatait az AE√Å-nak √©s Ausztr√°li√°nak 2009 els≈ë negyed√©v√©t≈ël 2010 els≈ë negyed√©v√©ig,
                        n√©zz√ºk meg, hogy kaptunk-e valami inform√°ci√≥t vissza, √©s ha igen, akkor mit:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">if response.status_code == 200:
	json = response.json()
	json.keys()
    # output: dict_keys(['header', 'dataSets', 'structure'])</code></pre>
                    <p class="text-lg font-normal pb-3">A kapott JSON adat SDMX form√°tumban van, amely statisztikai adatok
                        k√∂zl√©s√©re
                        volt tervezve. Nem a legintu√≠tivabb form√°tum, de ez gyakran megt√∂rt√©nik az adatokkal val√≥ munka sor√°n.
                        Szerencs√©re, majd a Pandas k√∂nyvt√°r orvosolja a probl√©m√°t, hiszen rendelkezik a pandaSDMX-el.</p>
                    <p class="text-lg font-normal pb-3">Van n√©h√°ny nemzeti statisztika, amely hasznos lesz sz√°munkra mik√∂zben a
                        v√©gs≈ë
                        projektet k√©sz√≠ts√ºk el. A lakoss√°g m√©rete, a 3 karakteres nemzetk√∂zi azononos√≠t√≥-k√≥dok √©s m√©g sok m√°s
                        egy√©b is
                        potenci√°lis hasznoss√° v√°lhat, amikor valamilyen nemzetk√∂zi d√≠jat √©s annak eloszt√°s√°t vizsg√°ljuk √©s
                        pr√≥b√°ljuk
                        megjelen√≠teni.</p>
                    <p class="text-lg font-normal">A <em>REST countries</em> egy nagyon k√∂nnyen kezelhet≈ë webes forr√°s, amely
                        k√ºl√∂nb√∂z≈ë nemzetk√∂zi statisztik√°kat tartalmaz. Haszn√°ljuk fel n√©mi adat lek√©r√©s√©re:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">REST_EU_ROOT_URL = 'http://restcountries.eu/rest/v1'
    
def REST_country_request(field='all', name=None, params=None):
    headers = { 'User-Agent' : 'Mozilla/5.0' }
    # legt√∂bbsz√∂r j√≥ √∂tlet az, ha meghat√°rozzunk egy valid 'User-Agent'-et a request fejl√©c√©ben,
    # annak √©rdek√©ben tessz√ºk ezt, hogy elker√ºljek azt az esetet, amikor nem lehet az adatot el√©rni meghat√°rozott fejl√©c n√©lk√ºl
    # a fejl√©cek nem m√°sok, mint kieg√©sz√≠t≈ë inform√°ci√≥k a HTTP csatlakoz√°s sor√°n
    # ebben az esetben az 'User-Agent' l√©nyeg√©ben inform√°ci√≥t szolg√°ltat az API-nak arr√≥l, hogy milyen rendszer k√©ri le az inform√°ci√≥t,
    # azonos√≠tva ez √°ltal
    	
    if not params:
    	params = {}

    if field == 'all':
            return requests.get(REST_EU_ROOT_URL + '/all')
        
    url = '{}/{}/{}'.format(REST_EU_ROOT_URL, field, name)
    print('Requesting URL: {}'.format(url))
    response = requests.get(url, params=params, headers=headers)
    # egy REST countries URL √≠gy n√©z ki: https://restcountries.eu/rest/v1/&#60;field>/&#60;name>?&#60;params>

    if not response.status_code == 200:
        raise Exception('Request failed with status code {}'.format(response.status_code))
    # miel≈ëtt visszat√©r√≠ten√©nk a v√°laszt, megn√©zz√ºk √©rv√©nyes-e

    return response</code></pre>
                    <p class="text-lg font-normal">Miut√°n megvan a f√ºggv√©ny, amellyel az OECD p√©ld√°hoz hasonl√≥an lek√©r√©seket
                        int√©zhet√ºnk az API-hoz, k√©rj√ºk le az √∂sszes orsz√°got, amelynek doll√°r a p√©nzneme:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">response = REST_country_request('currency', 'usd')
response.json()</code></pre>
                    <p class="text-lg font-normal">Tov√°bbi gyakorl√°sk√©ppen mivel a REST orsz√°gok adatsorja el√©g kicsi, csin√°ljunk
                        bel≈ële egy helyi m√°solatot √©s t√°roljuk MongoDB-n a nobel-prize adatb√°zisban, el√©rve az adatb√°zist a
                        kor√°bban
                        l√©trehozott f√ºggv√©ny√ºnket a <em>get_mongo_database</em>-t.</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">from pymongo import MongoClient

def get_mongo_database(db_name, host='localhost', port=27017, username=None, password=None):
    if username and password:
        mongo_uri = 'mongodb://{}:{}@{}/{}'.format(username, password, host, db_name)
        conn = MongoClient(mongo_uri)
    else:
        conn = MongoClient(host, port)

    return conn[db_name]

db = get_mongo_database('nobel_prize')
col = db['country_data'] # egy collection az orsz√°gadatokkal

# kor√°bbi l√©p√©sben lek√©rt√ºk √©s t√°roltuk a response v√°ltoz√≥ba a REST country request-et

col.insert_many(response.json())
# beillesztj√ºk a kapott JSON √°llom√°nyt a collectionbe</code></pre>
                    <p class="text-lg font-normal">Miut√°n elhelyezt√ºk az orsz√°gos adatokat a MongoDB collection-√ºkbe, k√©rj√ºk le
                        az
                        adatb√°zisb√≥l az √∂sszes olyan orsz√°got, ahol doll√°r a p√©nznem:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">res = col.find({'currencies': {'$in': ['USD']}})
list(res)</code></pre>
                    <p class="text-lg font-normal pb-3">Na m√°r most, miut√°n meg√≠rt√ºnk n√©h√°ny saj√°t API kezel≈ët, n√©zz√ºnk meg
                        n√©h√°ny
                        √∂sszetetebb API kezel√©s√©re dedik√°lt k√∂nyvt√°rat, amelyek a nagy API-kat k√∂nnyen kezelhet≈ëv√© teszik. A
                        requests
                        k√∂nyvt√°r majdnem minden API-val k√©pes egy√ºtt m≈±k√∂dni, √©s legt√∂bbsz√∂r csak olyan apr√≥ f√ºggv√©nyekre van
                        sz√ºks√©g,
                        mint amit a kor√°bbi p√©ld√°kban is l√°thattunk. De, ahogy az API-k is elkezdnek hiteles√≠t√©seket haszn√°lni,
                        √©s az
                        adatstrukt√∫ra is egyre bonyolultabb√° v√°lik, n√©ha j√≥l j√∂n egy kieg√©sz√≠t≈ë k√∂nyvt√°r, amely megk√∂nny√≠ti a
                        folyamatot.</p>
                    <p class="text-lg font-normal pb-3">Egyre gyakoribb manaps√°g, hogy az adatsorokat a felh≈ëben t√°rolj√°k.
                        P√©ldak√©ppen,
                        megt√∂rt√©nhet az, hogy az az adat, amit √°br√°zolni kellene Google Spreadsheet-ben tal√°lhat√≥, ami egy
                        csoporton
                        bel√ºl van megosztva. A legjobb megold√°s az, ha valamilyen m√≥don megszerezz√ºk az adatot √©s Pandas-el
                        vizsg√°ljuk
                        meg, de egy√©b lehet≈ës√©gek is rendelkez√©s√ºnkre √°llnak, mint pl. a Gspread k√∂nyvt√°r.</p>
                    <p class="text-lg font-normal pb-3">A Gspread k√∂nyvt√°r, az egyik legelterjedtebb, ami a Google
                        Spreadsheet-ekhez
                        val√≥
                        hozz√°f√©r√©st biztos√≠tja √©s viszonylag k√∂nnyedd√© teszi vel√ºk a munk√°t. Sz√ºks√©g√ºnk lesz OAuth 2.0
                        credentials-ra
                        ahhoz, hogy haszn√°lhassuk az API-t.R√∂viden √©s legegyszer≈±bb form√°j√°ban az OAuth egy olyan hozz√°f√©r√©si
                        standard, amely √∫gy teszi lehet≈ëv√© egy felhaszn√°l√≥ weboldalaknak vagy applik√°ci√≥knak az adatokhoz val√≥
                        hozz√°f√©r√©st, hogy nem adja meg mindek√∂zben sz√°mukra az el√©r√©si jelsz√≥t. A Google-nek egy hasznos kis
                        √∫tmutat√≥ja van ahhoz, hogy megszerezz√ºk a saj√°t priv√°t kulcsunkat az adatokhoz val√≥ hozz√°f√©r√©shez: <a
                            href="https://developers.google.com/identity/protocols/oauth2/service-account"
                            class="group text-lg pb-8 text-gray-600
                    hover:text-blue-800 opacity-75 hover:opacity-100 transition duration-100 ease-in">√∫tmutat√≥ <span class="inline-block font-normal transform group-hover:translate-x-1 transition duration-100
                    ease-in">‚Üó</span></a> .</p>
                    <p class="text-lg font-normal">Az √∫tmutat√≥ l√©p√©seit k√∂vetve, meg is kaptunk egy JSON √°llom√°nyt amely
                        tartalmazza a
                        priv√°t kulcsot. Ahhoz, hogy haszn√°lhassuk a k√∂nyvt√°rt telep√≠ten√ºnk kell, illet≈ëleg a legfrisebb OAuth2
                        klienssel is rendelkezn√ºnk kell:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="shell">$ pip install gspread
$ pip install --upgrade oauth2client</code></pre>
                    <p class="text-lg font-normal">A megszerzett credentials-al b√°rmely spreadsheet-et k√©pes kellene legy√©l
                        el√©rni
                        mind√∂ssze n√©h√°ny sorral. A k√∂vetkez≈ë p√©lda azt szeml√©lteti hogyan lehet bet√∂lteni egy ilyen
                        spreadsheet-et:
                    </p>
                    <pre
                        class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">import json
import gspread
from google.oauth2.service_account import Credentials

scope = ['https://spreadsheets.google.com/feeds',
		'https://www.googleapis.com/auth/drive']

# fontos megjegyezni azt, hogy mindk√©t fenti scope-ban tal√°lhat√≥ API-t enged√©lyezni kellett a projekt service account-j√°ra a Google APIs-b√≥l
# teh√°t a Google Sheets API-t, √©s a Google Drive API-t

# a credentials JSON √°llom√°ny a Google Services-t≈ël van
credentials = Credentials.from_service_account_file('../key/pyjsviz-282509-6cfa775f3db7.json', scopes=scope)

# ezzel hiteles√≠tj√ºk a hozz√°f√©r√©s√ºnket a Google Spreadsheet-ekhez
gc = gspread.authorize(credentials)

# megnyitjuk az URL-vel hivatkozott Google Spreadsheet-et
ss = gc.open_by_url('https://docs.google.com/spreadsheets/d/1kHCEWY-d9HXlWrft9jjRQ2xf6WHQlmwyrXel6wjxkW8/edit#gid=0')</code></pre>
                    <p class="text-lg font-normal">Miut√°n, sikeresen el√©rt√ºk a Google Spreadsheet-et Pythonb√≥l, l√©trehozva a
                        service
                        account-ot, let√∂ltve annak hiteles√≠t≈ë JSON √°llom√°ny√°t az applik√°ci√≥nk hiteles√≠t√©s√©hez, valamint
                        enged√©lyezve a
                        sz√ºks√©ges API-kat, ak√°r meg is n√©zhetj√ºk, hogy az el√©rt Spreadsheet milyen munkalapokkal rendelkezik:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">ss.worksheets()</code></pre>
                    <p class="text-lg font-normal">Ez ut√°n ak√°r egy munkalapot ki is v√°laszthatunk, hogy el√©rj√ºk a cell√°iban
                        tal√°lhat√≥
                        adatokat:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">ws = ss.worksheet('bugs')</code></pre>
                    <p class="text-lg font-normal">A k√∂vetkez≈ëkben ak√°r t√©nyleges le is k√©rhetj√ºk az adatokat, lek√©rve a teljes
                        els≈ë
                        oszlop c√©ll√°it:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">ws.col_values(1)</code></pre>
                    <p class="text-lg font-normal">Ak√°r gspread-b≈ël k√∂zvetlen√ºl √©s k√©pesek vagyunk √°br√°zolni a munkalapon
                        tal√°lhat√≥
                        inform√°ci√≥kat, de m√©gis jobb szakosodott k√∂nyvt√°rakat haszn√°lni erre, mint a Pandas-t. A gspread
                        get_all_records met√≥dus√°t haszn√°lva k√©pesek vagyunk az √∂sszes oszlopot lek√©rni, amivel majd k√©pesek
                        lesz√ºnk a
                        Pandas DataFrame-t inicializ√°lni:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">import pandas as pd

df = pd.DataFrame(ws.get_all_records())</code></pre>
                    <p class="text-lg font-normal pb-3">√Ått√©rve a k√∂vetkez≈ë k√∂nyvt√°rra, amelyet bonyolultabb API-kkal val√≥
                        munk√°kra
                        haszn√°lhatunk, besz√©ln√ºnk kell a Tweepy-r≈ël. A k√∂z√∂ss√©gi m√©dia f√©nykor√°ban rengeteg adat gener√°l√≥dik
                        ezeken a
                        platformokon. A Twitter platform 'broadcast' h√°l√≥zata egy nagyon gazdag adatforr√°s vizualiz√°ci√≥khoz √©s
                        annak
                        API-ja ak√°r sz≈±r√©sen √°tesett hozz√°f√©r√©st is biztos√≠t a tweet-ekhez, felhaszn√°l√≥kk√©nt, hashtagenk√©nt,
                        d√°tumonk√©nt, stb.
                    </p>
                    <p class="text-lg font-normal pb-3">A Tweepy egy k√∂nnyen haszn√°lhat√≥ Twitter k√∂nyvt√°r √©rtelemszer≈±en, amely
                        sz√°mos
                        hasznos feature-t biztos√≠t, mint p√©ld√°ul a StreamListener class-t, hogy √©l≈ëben streamelhess√ºk a Twitter
                        friss√≠t√©seket. Ahhoz, hogy haszn√°lhassuk el≈ësz√∂ris sz√ºks√©g√ºnk van egy Twitter access token-re, amely
                        el√©rhet≈ë
                        a k√∂vetkez≈ë √∫tmutat√≥ haszn√°lat√°val: <a
                            href="https://developer.twitter.com/en/docs/basics/authentication/oauth-1-0a/obtaining-user-access-tokens"
                            class="group text-lg pb-8 text-gray-600
                    hover:text-blue-800 opacity-75 hover:opacity-100 transition duration-100 ease-in">√∫tmutat√≥ <span class="inline-block font-normal transform group-hover:translate-x-1 transition duration-100
                    ease-in">‚Üó</span></a> .
                    </p>
                    <p class="text-lg font-normal pb-3">A k√∂nyvt√°r m≈±k√∂d√©s√©t nem tudom megvizsg√°lni, nem rendelkezek Twitter-rel,
                        de
                        az √∫tmutat√≥ja a k√∂vetkez≈ë linken el√©rhet≈ë: <a href="http://docs.tweepy.org/en/latest/"
                            class="group text-lg pb-8 text-gray-600
                    hover:text-blue-800 opacity-75 hover:opacity-100 transition duration-100 ease-in">√∫tmutat√≥ <span class="inline-block font-normal transform group-hover:translate-x-1 transition duration-100
                    ease-in">‚Üó</span></a> .
                    </p>
                    <p class="text-lg font-normal pb-3">K√∂vetkez≈ëkben besz√©ln√ºnk kell a 'Scrape-r≈ël'. Ezzel a sz√≥val jel≈ël√ºnk egy
                        folyamatot, amikor is √∫gy szerz√ºnk meg valamilyen adatot a netr≈ël, hogy val√≥j√°ban azt sosem tervezt√©k meg
                        erre. Alapvet≈ëen, ez a folyamat az√©rt lehet neh√©zkes sok esetben, mert ebben az esetben nek√ºnk kell
                        eld√∂nten√ºnk azt, hogy mit √©rdemes kihagyni, mi n√©lk√ºl√∂zhetetlen, teh√°t megkell tal√°lni az egyens√∫lyt.
                        Olyan
                        proced√∫r√°k l√©trehoz√°sa, amelyek pontosan a sz√ºks√©ges adatot szerzik meg, annyira tiszt√°n, amennyire csak
                        lehet, olyan weboldalakr√≥l, amelyek sok esetben k√∂vethetetlenek egy k√©pess√©g m√°r √∂nmag√°ban is.
                    </p>
                    <p class="text-lg font-normal pb-3">Az interneten egyar√°nt tal√°lhat√≥ olyan adat, amely √∂nmag√°ban tiszta √©s
                        haszn√°lhat√≥, de m√©gis a legt√∂bbje √∂nmag√°ban fogyaszthatatlan √©s felhaszn√°lhatatlan eredeti form√°j√°ban.
                        Ez√©rt
                        sz√ºks√©ges olyan kezel√©si folyamatokat l√©trehozni, amely az ember √°ltal m√©g √©ppen √©rthet≈ë
                        adat-halmazokb√≥l,
                        haszn√°lhat√≥ adatsorokat hoznak l√©tre, amelyeket k√©s≈ëbb √°br√°zolhatunk.
                    </p>
                    <p class="text-lg font-normal pb-3">Ebben a r√©szben egy ilyen probl√©m√°t fogunk megoldani, mik√∂zben
                        megpr√≥b√°ljuk megszerezni a Nobel-gy≈ëztesek list√°j√°t. A probl√©ma megold√°s√°hoz felhaszn√°ljuk a
                        BeautifulSoup k√∂nyvt√°rat, de az ezt k√∂vet≈ë fejezetben egy √∫jabb Scrapel√©sre szakosodott k√∂nyvt√°rat fogunk
                        megvizsg√°lni, n√©vszerint a Scrapy-t.
                    </p>
                    <p class="text-lg font-normal pb-3">Alapvet≈ëen, a Pythonnak k√©t lightweight scrapel√©sre szakosodott k√∂nyvt√°ra
                        van: BeautifulSoup √©s lxml. Az els≈ëdleges szintaxisuk, ami a kiv√°laszt√°st illeti k√ºl√∂nb√∂zik, de, hogy
                        j√∂bben √∂sszezavarja a dolgok √°ll√°s√°t, mindk√©t k√∂nyvt√°r k√©pes egym√°s √©rtelmez≈ëj√©t (parser) haszn√°lni. Az
                        √°ltal√°nos megegyez√©s √∫gy √°ll, hogy az lxml √©rtelmez≈ëje gyorsabb, azonban a BeautifulSoup sokkal jobban
                        kezeli a rosszul meg√≠rt HTML-t. Ugyanakkor, az lxml kiv√°laszt√°sa, amely xpath-re alapszik, sokkal jobban
                        hasonl√≠t a jQuery √©s CSS
                        √°ltal haszn√°lt selectorokhoz, √≠gy webfejleszt≈ëk sz√°m√°ra az tal√°n egy intu√≠tivabb eszk√∂z lenne.
                    </p>
                    <p class="text-lg font-normal">El≈ësz√∂ris, n√©zz√ºk meg a BeautifulSoup k√∂nyvt√°rat. Amelyet k√∂nnyed√©n
                        feltelep√≠thet√ºnk a pip seg√≠ts√©g√©vel:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="shell">$ pip install beautifulsoup4</code></pre>
                    <p class="text-lg font-normal pb-3">Minden rendelkez√©s√ºnkre √°ll, hogy megszerezz√º√∂k magunknak a neveiket,
                        nyer√©s√ºknek √©v√©t, kategori√°jukat √©s v√©g√ºl nemzetis√©g√ºket a Nobel-gy≈ëzteseknek, mind√∂ssze BeautifulSoup-ot
                        √©s requests-et haszn√°lva. Kiindul√°sk√©ppen haszn√°ljuk a Wikip√©dia Nobel-d√≠j oldal√°t, ahol egy t√°bl√°zatban
                        felvannak sorolva az eddigi gy≈ëztesek.
                    </p>
                    <figure>
                        <img class="mx-auto"
                            src="../img/fig2.png"
                            alt="√Åbra 2">
                        <figcaption class="text-center pt-3 pb-4 text-md">2. √Åbra: Nobel-gy≈ëztesek list√°ja</figcaption>
                    </figure>
                    <p class="text-lg font-normal pb-3">Sz√ºks√©g√ºnk van valamilyen HTML kezel≈ëre ahhoz, hogy megvizsg√°ljuk mib≈ël is
                        √©p√ºl fel a weboldal, term√©szetesen a Chrome DevTools t√∂k√©letes a feladatra. Alapvet≈ëen, azt sz√ºks√©ges
                        megfigyeln√ºnk, hogy hogyan √©p√ºl fel a weboldal, hogyan tudjuk kiv√°lasztani a √©rdekelt adatokat, mik√∂zben
                        elker√ºlj√∂k a felesleges tartalmi elemeket. A jelenesetben, a t√°bl√°zat az √©rdekes a sz√°munkra. Az eg√©sz
                        scrapel√©si folyamat arra alapszik, hogy k√©pesek vagyunk-e hat√©kony megalkotni a selectorunkat, amellyel
                        pontosan azt az adatot √©rj√ºk el a weboldalon, amire sz√ºks√©g√ºnk van.
                    </p>
                    <p class="text-lg font-normal pb-3">Az el≈ëbb eml√≠tettem, hogy a lxml kiv√°laszt√≥ja xpath-re alapszik, amit
                        nat√≠van biztos√≠t sz√°munkra a Chrome DevTools. Ahhoz, hogy el√©rj√ºk egy HTML elem xpath-j√©t, az Elements
                        tab-on bel√ºl az elemre jobb-klikkelve √©s a Copy kontext-men√ºpontot kiv√°lasztva el√©rhetj√ºk az xpath
                        el√©r√©s√©t, ami valahogy √≠gy n√©z ki: <em>//*[@id="mw-content-text"]/div/table[1]</em>.
                    </p>
                    <figure>
                        <img class="mx-auto"
                            src="../img/fig3.png"
                            alt="√Åbra 3">
                        <figcaption class="text-center pt-3 pb-4 text-md">3. √Åbra: Nobel-gy≈ëztesek list√°j√°nak xpath-je
                        </figcaption>
                    </figure>
                    <p class="text-lg font-normal">Az els≈ë dolog, amit tenn√ºnk kell miel≈ëtt t√©nyleges scrapelhetn√©nk a
                        weboldal az, hogy √©rtelmezz√ºk a weboldalt a BeautifulSoup seg√≠ts√©g√©vel, √°talak√≠tva a HTML-t egy faszer≈±
                        elrendez√©sbe, amely a HTML tag-ekb≈ël √°ll (tag tree hierarchy), a k√∂nyvt√°r eset√©ben ezt Soup-nak szok√°s
                        h√≠vni.
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">from bs4 import BeautifulSoup
import requests

BASE_URL = 'https://en.wikipedia.org'
# A Wikip√©dia eset√©ben m√°r fontos, hogy meghat√°rozzuk a 'User-Agent' fejl√©cet, amely alapj√°n azonos√≠tani tudja azt, hogy
# milyen rendszerb≈ël pr√≥b√°ljuk el√©rni a weboldal tartalm√°t
HEADERS = { 'User-Agent': 'Mozilla/5.0' }

def get_Nobel_soup():
	""" Visszat√©r√≠ti az √©rtelmezett tag f√°t a Nobel-d√≠jas weboldalunkr√≥l """
	# egy lek√©r√©st int√©z a Nobel-d√≠jas oldalhoz, be√°ll√≠tva a valid Headers fejl√©cet

	response = requests.get('{}/wiki/List_of_Nobel_laureates'.format(BASE_URL), headers=HEADERS)

	if not response.status_code == 200:
		raise Exception('Something wrong happened with status code: {}'.format(response.status_code))

	return BeautifulSoup(response.content, 'lxml')
	# ebben az esetben m√°r megmutatkozik az, hogy a k√©t k√∂nyvt√°r k√©pes egym√°s parserj√©t haszn√°lni,
        # a m√°sodik argumentum a parsert lxml-re √°ll√≠tja</code></pre>
                    <p class="text-lg font-normal">Miut√°n megvan a soup, n√©zz√ºk meg hogyan tudjuk megtal√°lni a c√©ltageket,
                        amelyre sz√ºks√©g√ºnk van az √©rdemi adat kinyer√©s√©hez. A BeautifulSoup n√©h√°ny m√≥dot biztos√≠t arra, hogy
                        tageket v√°lasszunk ki egy m√°r parselt soup-b√≥l. Megfigyelhetj√ºk az Chrome DevTools Elements tabj√©ben azt,
                        hogy a t√°bl√°zatuknak k√©t fontos classja van, n√©vszerint a <em>wikitable</em> √©s <em>sortable</em>.
                        Haszn√°lhatjuk a k√∂nyvt√°r find met√≥dus√°dat, hogy megtal√°ljuk az els≈ë table taget az eml√≠tett classokkal. A
                        met√≥dus beveszi a tag nev√©t, mint els≈ë argumentum, √©s egy dict-et a clasokkal, id-kkal √©s m√°s egy√©b
                        azonos√≠t√≥kkal. A k√∂vetkez≈ë k√≥dr√©szlet ezt szeml√©lteti:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">soup = get_Nobel_soup()

soup.find('table', {'class': 'wikitable sortable'})</code></pre>
                    <p class="text-lg font-normal pb-3">Hab√°r, a fenti p√©lda ebben az esetben m≈±k√∂dik, m√©gsem robusztus a
                        megold√°s,
                        el√©g, ha csak a classok sorrendj√©t megv√°ltoztatjuk a dict-en bel√ºl, √©s m√°ris m√°s eredm√©nyt fogunk kapni.
                        Ez√©rt mondhat√≥ az el, hogy a BeautifulSoup selectorja al√∫lmarad az lxml selectorj√°t√≥l.
                    </p>
                    <p class="text-lg font-normal">Felhaszn√°lva a soup select met√≥dus√°t, amely csak akkor √°ll rendelkez√©s√ºnkre, ha
                        az lxml-t hat√°roztuk meg annak parserjek√©nt, k√©pes vagy meghat√°rozni egy HTML elemet a CSS classait
                        felhaszn√°lva. Ezeket a CSS selectorokat felhaszn√°lva az xpath szintaxtisba fogja √°talak√≠tani az lxml
                        teljesen bels≈ëleg, b√°rmif√©le t√∂bbletl√©p√©s n√©lk√ºl. Tekints√ºk meg a k√∂vetkez≈ë k√≥dr√©szletet:
                    </p>
                    <pre
                        class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">soup.select('table.sortable.wikitable')
# ugyanazt az eredm√©nyt kapjunk, mint az el≈ëbb, azonban ebben az esetben nem okoz j√∂v≈ëbeli gondot az, ha a sorrend megv√°ltozik</code></pre>
                    <p class="text-lg font-normal">Az el≈ëbbi esetben egy list√°nyi elemet t√©r√≠tett vissza a select met√≥dus,
                        amennyiben egyetlen elemet szeretn√©k kiv√°lasztani haszn√°lhatjuk a select_one met√≥dust:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">table = soup.select_one('table.sortable.wikitable')
table.select('th')
# kiv√°lasztja az √∂sszes table-header elemet a t√°bl√°zaton bel√ºl</code></pre>
                    <p class="text-lg font-normal">√ârdekess√©g:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">table.select('th')
# a k√©t jel√∂l√©s ekvivalens
table('th')</code></pre>
                    <p class="text-lg font-normal pb-3">Az lxml parserj√©vel a BeautifulSoup k√∂nyvt√°r sz√°mos lehet≈ës√©get biztos√≠t
                        arra,
                        hogy sz≈±rj√ºk a keresett tageket, az el≈ëbb haszn√°lt egyszer≈± string n√©v mellett, ak√°r haszn√°lhatunk
                        regExp-et keres√©skor, egy list√°nyi tag nevet is, de egy√©b m√≥dok is vannak, a k√∂vetkez≈ë √∫tmutat√≥
                        tartalmazza a lehets√©ges sz≈±r√©si lehet≈ës√©geket: <a
                            href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#kinds-of-filters"
                            class="group text-lg pb-8 text-gray-600
                    hover:text-blue-800 opacity-75 hover:opacity-100 transition duration-100 ease-in">√∫tmutat√≥ <span class="inline-block font-normal transform group-hover:translate-x-1 transition duration-100
                    ease-in">‚Üó</span></a> .
                    </p>
                    <p class="text-lg font-normal pb-3">Miut√°n megtanultuk azt hogyan tudjuk kiv√°lasztani a Wikip√©dia
                        t√°bl√°zatunkat √©s
                        felfegyverkezt√ºnk az lxml kiv√°laszt√°si met√≥dusaival, n√©zz√ºk meg hogyan tudunk √∂sszehozni n√©h√°ny
                        kiv√°laszt√°si mint√°t (selection pattern), hogy megszerezz√ºk azt az adatot, amire sz√ºks√©g√ºnk van.
                    </p>
                    <p class="text-lg font-normal">K√∂vetkez≈ë l√©p√©shez, megkell vizsg√°lnunk azt, hogy hogyan is van megjelen√≠tve az
                        adat a HTML strukt√∫r√°ban. Minden egyes gy≈ëztes egy <em>&#60;td></em> elem, amelyen bel√ºl egy hyperlink a
                        gy≈ëztes Wikip√©dia √©letrajzi oldal√°ra mutat, egy√©nek eset√©ben legal√°bbis. Alapvet≈ëen, teh√°t nem m√°st
                        kellene csin√°lnunk, mint v√©gigmenni a sorokon, illet≈ëleg oszlopokon, amik a kateg√≥ri√°t jel≈ëlik, lementve
                        az √©vet, illet≈ëleg a gy≈ëztesek nev√©t. Tekints√ºk meg a k√∂vetkez≈ë k√≥dr√©szletet:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">def get_column_titles(table):
	""" Megszerzi a Nobel kateg√≥ri√°kat a table-headerekb≈ël """

	cols = []

	# [1:]-vel kihagyjuk az els≈ë table-headert, mert az ugye csak a 'year'-t tartalmazza
	for th in table.select_one('tr').select('th')[1:]:
		link = th.select_one('a')
		# t√°rolja a kateg√≥ria nev√©t √©s b√°rmely Wikip√©dia linket, amit tartalmaz

		if link:
			cols.append({'name': link.text, 'href':link.attrs['href']})
		else:
			cols.append({'name': th.text, 'href': None})

    return cols</code></pre>
                    <p class="text-lg font-normal">Ak√°r meg is bizonyosodhatunk arr√≥l, hogy a f√ºggv√©ny val√≥ban j√≥l m≈±k√∂dik:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">print(get_column_titles(table))</code></pre>
                    <p class="text-lg font-normal pb-3">Teh√°t, m√°r alapvet≈ëen ismerj√ºk a t√°bl√°zat fel√©p√≠t√©s√©t, √©s ki is nyert√ºk
                        bel≈ële
                        az els≈ë relev√°ns adatot.
                    </p>
                    <figure>
                        <img class="mx-auto"
                            src="../img/fig4.png"
                            alt="√Åbra 4">
                        <figcaption class="text-center pt-3 pb-4 text-md">4. √Åbra: Nobel-gy≈ëztesek list√°j√°nak fel√©p√≠t√©se
                        </figcaption>
                    </figure>
                    <p class="text-lg font-normal">Azt√°n √≠runk egy f√ºggv√©nyt, amely a gy≈ëzteseket is kinyeri sz√°munkra:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">def get_Nobel_winners(table):
    cols = get_column_titles(table)

    winners = []

    # kihagyjuk az els≈ë √©s utols√≥ t√°bl√°zat sort, mert az a kateg√≥ri√°kat √©s a 'year'-t tartalmazza
    for row in table.select("tr")[1:-1]:
        year = int(row.select_one("td").text)  # lek√©ri az els≈ë td-t, amiben az √©v van

        for i, td in enumerate(row.select("td")[1:]):
            for winner in td.select("a"):
                href = winner.attrs["href"]
                if not href.startswith("#endnote"):
                    winners.append(
                        {
                            "year": year,
                            "category": cols[i]["name"],
                            "name": winner.text,
                            "link": winner.attrs["href"],
                        }
                    )

    return winners</code></pre>
                    <p class="text-lg font-normal">√ñsszes√©g√©ben, a fenti f√ºggv√©ny eset√©ben nem m√°st tesz√ºnk, mint el≈ësz√∂ris
                        megszerezz√ºk az √©vet, amelyre az adott sor vonatkozik, majd ezt az oszlopot kihagyva, a t√∂bbin v√©gig
                        iter√°lva lementj√ºk a gy≈ëzteseket. Az iter√°ci√≥ eset√©ben az√©rt haszn√°lunk enumerate-ot, mert √≠gy az indexet
                        is nyomon k√∂vethetj√ºk, ami fontos, a megfelel≈ë kateg√≥ria t√°rs√≠t√°shoz. Az #endnote ellen≈ërz√©shez az√©rt van
                        sz√ºks√©g, mert ilyen tartalm√∫ hyperlinkkel rendelkez≈ë anchor tagek is tal√°lhat√≥k a t√°bl√°zatban. A winner
                        selector rendelkezik egy attrs dict-el, amely egy√©b dolgok mellett tartalmazza az anchor tag linkj√©t is.
                        Ellen≈ër√≠zz√ºk le megfelel≈ëen kisz≈±rte-e a Nobel-gy≈ëzteseket a f√ºggv√©ny:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">print(get_Nobel_winners(table))</code></pre>
                    <p class="text-lg font-normal pb-3">Megvan a Nobel-gy≈ëztesek teljes list√°ja, √©s mindegyik gy≈ëztesnek a
                        Wikip√©dia-√©letrajz√°nak a hyperlinkje. Felhaszn√°ljuk mostm√°r ezeket a linkeket, hogy megszerezz√ºk az
                        egy√©nek √©letrajz√°nak sz√∂veg√©t is, ugyanakkor mivel ez egy hosszadalmas √©s er≈ëforr√°sig√©nyes folyamat, jobb
                        csak egyszer v√©grehajtani, teh√°t a probl√©m√°t √∫gy oldjuk meg, hogy a scrapelend≈ë adatot cache-elj√ºk, amely
                        lehet≈ëv√© teszi, hogy √∫gy hajtsuk rajta v√©gre k√ºl√∂nb√∂z≈ë scrapel√©si kis√©rleteket, m√≠g eljutunk a t√©nyleges
                        megold√°shoz, hogy nem kell visszat√©rn√ºnk √∫jra a Wikip√©dia oldal√°hoz.
                    </p>
                    <p class="text-lg font-normal">Szerencs√©re, ez k√∂zelsem olyan bonyolult folyamat, mint amilyennek t≈±nik,
                        haszn√°lhatjuk erre a requests-cache plugint, a requests-hez, mind√∂ssze n√©h√°ny sorral a legalapabb
                        cachel√©si sz√ºks√©gleteket k√©pes ez kiel√©g√≠teni. Feltelep√≠thetj√ºk a k√∂vetkez≈ë parancssori k√≥ddal:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="shell">$ pip install --upgrade requests-cache</code></pre>
                    <p class="text-lg font-normal">Itt tekinthet≈ë meg a legegyszer≈±bb form√°ja a requests-cache-nek:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">import requests
import requests_cache

requests_cache.install_cache()
# haszn√°ljuk a tov√°bbiakban a szok√°sos m√≥don a requests k√∂nyvt√°rat ...</code></pre>
                    <p class="text-lg font-normal">Az install_cache met√≥dus j√≥n√©h√°ny hasznos be√°ll√≠t√°ssal is rendelkezik,
                        amelyekben olyan dolgokat tudunk meghat√°rozni, mint p√©ld√°ul a backend, hol ker√ºlj√∂n t√°rol√°sra a cache,
                        p√©ld√°ul sqlite, memory, mongodb, stb, vagy ak√°r lej√°rati id≈ët is meghat√°rozhatunk neki m√°sodpercekben,
                        stb. A k√∂vetkez≈ë be√°ll√≠t√°s p√©ld√°ul elind√≠tja a cachel√©st sqlite-ra mentve, 2 √≥r√°s lej√°rati id≈ëvel:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python"># elind√≠t egy nobel_pages nev≈± cachet, sqlite backend-el √©s 2 √≥r√°s lej√°rati id≈ëvel
requests_cache.install_cache('nobel_pages', backend='sqlite', expire_after=7200)</code></pre>
                    <p class="text-lg font-normal">Teh√°t, m√°r minden a hely√©n van ahhoz, hogy megszerezz√ºk a gy≈ëztesek
                        nemzetis√©g√©t, jelenleg csak az els≈ë √∂tven√©t, pr√≥b√°lkoz√°sk√©ppen. El≈ësz√∂ris, figyelj√ºk meg hogyan √©p√ºl fel
                        megint az adat, amit megakarn√°nk szerezni.
                    </p>
                    <figure>
                        <img class="mx-auto"
                            src="../img/fig5.png"
                            alt="√Åbra 5">
                        <figcaption class="text-center pt-3 pb-4 text-md">5. √Åbra: Nobel-gy≈ëztes nemzetis√©ge
                        </figcaption>
                    </figure>
                    <p class="text-lg font-normal">A k√∂vetkez≈ë f√ºggv√©ny veszi egyes√©vel a gy≈ëzteseket a kor√°bban kapott dict-ekb≈ël
                        √©s visszat√©r√≠t egy n√©v-c√≠mk√©zett dict-et a 'Nationality' kulccsal, ha tal√°lt:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">def get_winner_nationality(w):
	""" Scrapelj√ºk az √©letrajzi adatokat a gy≈ëzt√©s wikip√©dia oldal√°r√≥l """

	data = requests.get(BASE_URL + w['link'], headers=HEADERS)
	soup = BeautifulSoup(data.content, 'lxml')

	person_data = {'name': w['name']}
	attr_rows = soup.select('table.infobox tr')

	for tr in attr_rows:
		try:
			attribute = tr.select_one('th').text
			if attribute == 'Nationality':
				person_data[attribute] = tr.select_one('td').text
		except AttributeError:
			pass

	return person_data

wdata = []

for w in ws[:50]:
	wdata.append(get_winner_nationality(w))

missing_nationality = []

for w in wdata:
	if not w.get('Nationality'):
		missing_nationality.append(w)

print(missing_nationality)</code></pre>
                    <p class="text-lg font-normal">Az utols√≥ teszt r√°vil√°g√≠t arra, hogy nem minden esetben volt sikeres. A
                        probl√©ma oka az, hogy nincs egy szabv√°ny √©letrajzi forma, vagy ak√°r szinonim√°kat is haszn√°lhatnak, amely
                        tov√°bbi bonyodalmakat okoz. Alapvet≈ëen, √≠gy azt lehet kik√∂vetkeztetni, hogy gyakran a megfelel≈ë minta
                        megtal√°l√°s√°hoz relat√≠ve sok tesztre √©s pr√≥b√°lkoz√°sra van sz√ºks√©g√ºnk.
                    </p>
                </section>
            </section>

            <!-- harmadik r√©sze v√©ge -->

            <div class="my-8 flex content-center">
                <button class="flex-auto w-24 bg-gray-900 text-white group hover:bg-blue-800 transition duration-75 ease-in"><a
                        class="block px-4 py-2"
                        href="2.html"><span class="inline-block font-normal transform group-hover:-translate-x-1 transition
                            duration-100 ease-in">&larr;</span>
                        El≈ëz≈ë</a></button>
                <!--<button
                    class="flex-auto w-24 ml-4 py-2 bg-gray-900 text-white group hover:bg-blue-800 transition duration-75
                    ease-in"><a class="block px-4"
                        href="#">K√∂vetkez≈ë
                        <span
                            class="inline-block font-normal transform group-hover:translate-x-1 transition duration-100 ease-in">&rarr;</span></a></button>-->
            </div>

        </main>

        <div id="arr"
            class="fixed hidden lg:block transition duration-100 ease-in"><a
                class="inline-block text-3xl hover:text-blue-800 transition duration-100 ease-in transform hover:scale-125"
                href="#menu">&uarr;</a></div>

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/underscore.js/1.10.2/underscore-min.js"></script>
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
        <script src="../tools/pushy-1.3.0/js/pushy.min.js"></script>
        <script>
            hljs.initHighlightingOnLoad();
        </script>
        <!-- cross-platform megold√°s a smooth-scroll-ra, jQuery haszn√°lat√°val -->
        <script src="../javascript/smooth.js"></script>
        <script src="../javascript/main.js"></script>
        <script src="../javascript/introd3.js"></script>
    </body>

</html>