<!DOCTYPE html>
<html lang="en"
    class="text-black text-base antialiased leading-relaxed tracking-normal break-normal bg-white">

    <head>
        <meta charset="UTF-8">
        <meta name="viewport"
            content="width=device-width, initial-scale=1.0">
        <meta name="description"
            content="A repo for studying a book.">
        <meta name="keywords"
            content="visualization,javascript,python,d3,flask,restful">
        <meta name="author"
            content="Luk√°cs Zsolt">
        <meta http-equiv="X-UA-Compatible"
            content="ie=edge">
        <title>3 Az adatok megszerz√©se</title>
        <link rel="stylesheet"
            href="../css/style.css">
        <!-- Safari-n az emoji favicon nem m≈±k√∂dik -->
        <link rel="icon"
            href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22
                viewBox=%220
                0
                100
                100%22><text y=%22.9em%22
                    font-size=%2290%22>üìï</text></svg>">
    </head>

    <body class="overflow-x-hidden">
        <!-- navig√°ci√≥ -->
        <nav class="pushy pushy-left">
            <div class="pushy-content">
                <ul>
                    <li class="pushy-link"><a href="../index.html">1 Bevezet√©s</a></li>
                    <li class="pushy-submenu">
                        <button>2 Els≈ë l√©p√©sek az adatvizualiz√°ci√≥ban</button>
                        <ul>
                            <li class="pushy-link"><a href="2.html#2adviz">Az adatvizualiz√°ci√≥ folyamata</a></li>
                            <li class="pushy-link"><a href="2.html#2prog">A programoz√°si nyelvek, amelyekkel dolgozunk</a>
                            </li>
                            <li class="pushy-link"><a href="2.html#2progkul">K√ºl√∂nbs√©geik a gyakorlati felhaszn√°l√°sban</a>
                            </li>
                            <li class="pushy-link"><a href="2.html#2python">Adatok √≠r√°sa √©s olvas√°sa Python-al</a>
                            </li>
                            <li class="pushy-link"><a href="2.html#2webd">Webdev 101</a>
                            </li>
                        </ul>
                    </li>
                    <li class="pushy-submenu">
                        <button>3 Az adatok megszerz√©se</button>
                        <ul>
                            <li class="pushy-link"><a href="3.html#3python">Szerezz√ºk meg az adatot a netr≈ël Python-nal</a>
                            </li>
                            <li class="pushy-link"><a href="3.html#3scrapy">A komoly scrapel√©s Scrapy-vel</a></li>
                        </ul>
                    </li>
                    <li class="pushy-submenu">
                        <button>4 Az adatok tiszt√≠t√°sa √©s feldolgoz√°sa</button>
                        <ul>
                            <li class="pushy-link"><a href="4.html#4numpy">Bevezet√©s a NumPy-ba</a></li>
                            <li class="pushy-link"><a href="4.html#4pandas">Bevezet√©s a Pandas-ba</a></li>
                            <li class="pushy-link"><a href="4.html#4cleaning">Tiszt√≠tsuk meg az adatunkat Pandas-al</a></li>
                            <li class="pushy-link"><a href="4.html#4visualizing">Vizualiz√°ljuk adatunkat Matplotlib-el</a>
                            </li>
                            <li class="pushy-link"><a href="4.html#4explore">Fedezz√ºk fel az adatunkat Pandas-al</a></li>
                        </ul>
                    </li>
                    <li class="pushy-submenu">
                        <button>5 Az adatok eljuttat√°sa</button>
                        <ul>
                            <li class="pushy-link"><a href="5.html#5staticvsdynamic">Az adatok eljuttat√°sa</a></li>
                            <li class="pushy-link"><a href="5.html#5datawithflask">Folytassuk a RESTful API-t</a></li>
                        </ul>
                    </li>
                    <li class="pushy-submenu">
                        <button>6 Az adatok √°br√°zol√°sa</button>
                        <ul>
                            <li class="pushy-link"><a href="6.html#6imaginingnobelvizualization">Tervezz√ºk meg a
                                    vizualiz√°ci√≥nkat</a></li>
                            <li class="pushy-link"><a href="6.html#6buildingvizualization">√âp√≠ts√ºk meg a
                                    vizualiz√°ci√≥nkat</a></li>
                            <li class="pushy-link"><a href="6.html#6d3bar">Munka a D3-al √©s az oszlopdiagram</a></li>
                            <li class="pushy-link"><a href="6.html#6d3time">Az id≈ëvonal</a></li>
                            <li class="pushy-link"><a href="6.html#6d3map">T√©rk√©pk√©sz√≠t√©s a D3-al</a></li>
                            <li class="pushy-link"><a href="6.html#d3individual">Az egy√©ni nyertesek vizualiz√°ci√≥ja</a></li>
                            <li class="pushy-link"><a href="6.html#d3menu">A men√ºbar</a></li>
                        </ul>
                    </li>
                    <li class="pushy-submenu">
                        <button>7 V√©gs≈ë sim√≠t√°sok</button>
                        <ul>
                            <li class="pushy-link"><a href="7.html#7adatok">Els≈ë hiba: a Nobel-adatok</a></li>
                            <li class="pushy-link"><a href="7.html#7design">M√°sodik hiba: a Nobel-vizaliz√°ci√≥ diz√°jna</a></li>
                        </ul>
                    </li>
                </ul>
            </div>
        </nav>

        <!-- Pushy-hoz site overlay -->
        <div class="site-overlay"></div>

        <main id="container"
            class="container p-8 lg:px-6 lg:pb-6 lg:pt-2 max-w-2xl md:max-w-4xl min-h-screen mx-auto xl:ml-64">
            <button id="menu"
                class="menu-btn px-4 pb-2 pt-6 bg-gray-900 text-white hover:bg-blue-800 transition duration-75 ease-in">&#9776;
                Men√º</button>

            <!-- harmadik r√©sze eleje -->

            <section class="pb-8">
                <h1 class="text-3xl md:text-4xl py-8 text-gray-900 font-semibold">3 Az adatok megszerz√©se</h1>

                <section id="3python">
                    <h2 class="text-2xl pb-4 font-semibold text-gray-800"
                        role="button"
                        tabindex="0"
                        aria-pressed="false">Szerezz√ºk meg az adatot a netr≈ël Python-nal</h2>
                    <p id="firstPar"
                        class="text-lg font-normal pb-3">A k√∂nyv ennek a r√©sz√©ben nekikezd√ºnk a val√≥di adat-vizualiz√°ci√≥s
                        folyamatnak
                        az
                        adatokkal megszerz√©s√©vel.</p>
                    <p class="text-lg font-normal pb-3">Alapvet≈ëen elmondhat√≥, hogy n√©lk√ºl√∂zhetetlen r√©sze az
                        adat-megjelen√≠t√©snek
                        maga
                        az
                        adat, ami nem minden esetben √°ll k√©szen sz√°munkra. A lehet≈ë legjobb √©s legtiszt√°bb m√≥don kell azt
                        megszerezn√ºnk √©s ebben nagy seg√≠ts√©g√ºnkre van a Python. Sz√°mos Python k√∂nyvt√°r √°ll rendelkez√©s√ºnkre a
                        folyamat
                        v√©grehajt√°s√°ra. A legf≈ëbb m√≥dok arra ahogyan adatot tudunk let√∂lteni a netr≈ël: HTTP-n kereszt√ºl
                        megszereezz√ºk
                        a nyers adatf√°jlt, valamilyen ismert form√°tumban, legyen az CSV vagy JSON; ak√°r egy erre dedik√°lt API-t
                        is
                        haszn√°lhatunk; megszerezz√ºk az adatot az √°ltal, hogy let√∂ltj√ºk a weboldalt HTTP-n kereszt√ºl √©s
                        √©rtelmezz√ºk
                        helyileg a sz√ºks√©ges adatok√©rt.</p>
                    <p class="text-lg font-normal">El≈ësz√∂ris, n√©zz√ºk meg az egyik legelterjedtebb Python HTTP k√∂nyvt√°rt a
                        requests-et. Ahogyan megbesz√©lt√ºk a k√∂nyv el≈ëz≈ë r√©sz√©ben azon f√°jlok, amelyek a b√∂ng√©sz≈ëben megjelennek
                        val√≥j√°ban HTTP-n kereszt√ºl ker√ºlnek tov√°bb√≠t√°sra. Teh√°t, a webes tartalmat el≈ësz√∂r lekell k√©rni egy GET
                        request √°ltal, miel≈ëtt azt √©rtelmezni lehessen. Mivel a requests nem r√©sze a Python szabv√°ny k√∂nyvt√°rnak
                        el≈ësz√∂r
                        le kell azt t√∂lts√ºk a k√∂vetkez≈ë paranccsal:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="shell">$ pip install requests</code><button id="copy"
                            class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Miut√°n let√∂lt√∂tt√ºk neki is foghatunk a folyamatnak. Els≈ë l√©p√©sben pr√≥b√°ljuk a
                        k√∂nyvt√°r seg√≠ts√©g√©vel let√∂lteni egy Wikip√©dia oldalt. Haszn√°lhatjuk erre a requests k√∂nyvt√°r get
                        met√≥dus√°t,
                        amit a k√∂vetkez≈ë k√≥dr√©szlet is szeml√©ltet:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">import requests

response = requests.get('https://en.wikipedia.org/wiki/Nobel_Prize')

response # output: &#60;Response [200]>

dir(response) # output: a response objektum attrib√∫tumjai</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">A fenti 200-as HTTP st√°tusz k√≥d l√©nyeg√©ben a HTTP megfelel≈ëje a 'minden
                        rendben
                        volt'-nak. Ebben a kontextusban ez azt jelenti, hogy bizonyosan valamilyen inform√°ci√≥t a GET request
                        visszat√©r√≠tett, a k√∂vetkez≈ë paranccsal ak√°r meg is vizsg√°lhatjuk mit:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                            class="python">response.headers</code><button id="copy"
                            class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">N√©h√°ny nagyon √©rdekes dolgot tudunk megfigyelni a headers attrib√∫tummal,
                        p√©ld√°ul
                        azt, hogy a weboldalt tartalma <em>text/html</em>, a tartalom enk√≥dol√°sa <em>gzip</em>, stb:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">{'Date': 'Sun, 05 Jul 2020 15:09:55 GMT', 'Server': 'mw1407.eqiad.wmnet', 
'X-Content-Type-Options': 'nosniff', 'P3p': 'CP="See https://en.wikipedia.org/wiki/Special:CentralAutoLogin/P3P for more info."', 'Content-Language': 'en', 'Vary': 'Accept-Encoding,Cookie,Authorization', 'Last-Modified': 'Sun, 05 Jul 2020 14:58:26 GMT', 
'Content-Type': 'text/html; charset=UTF-8', 'Content-Encoding': 'gzip', 'Age': '10136', 'X-Cache': 'cp3054 miss, cp3064 hit/19', 'X-Cache-Status': 'hit-front', 'Server-Timing': 'cache;desc="hit-front"', 'Strict-Transport-Security': 'max-age=106384710; includeSubDomains; preload', 
'Set-Cookie': 'WMF-Last-Access=05-Jul-2020;Path=/;HttpOnly;secure;Expires=Thu, 06 Aug 2020 12:00:00 GMT, WMF-Last-Access-Global=05-Jul-2020;Path=/;Domain=.wikipedia.org;HttpOnly;secure;Expires=Thu, 06 Aug 2020 12:00:00 GMT, GeoIP=MD:CU:Chisinau:47.01:28.86:v4; Path=/; secure; Domain=.wikipedia.org', 
'X-Client-IP': '178.17.168.163', 'Cache-Control': 'private, s-maxage=0, max-age=0, must-revalidate', 
'Accept-Ranges': 'bytes', 'Content-Length': '81973', 'Connection': 'keep-alive'}</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Mivel, a fenti vizsg√°lattal megtudtuk azt, hogy az eredm√©ny egy sz√∂veg, ak√°r
                        le is
                        k√©rhetj√ºk a val√≥di tartalm√°t:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                            class="python">response.text</code><button id="copy"
                            class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">A fenti k√≥dr√©szlet visszat√©r√≠ti a GET requests √°ltal lek√©rt teljes HTML
                        sz√∂veget.
                        √ñsszes√©g√©benn a requests egy el√©g k√©nyelmes megold√°s arra, hogy gyorsan szerezz√ºnk webes adatot a Python
                        programukba. P√©ldak√©ppen k√©rj√ºk le egy adatsettet az Eurostatr√≥l, kicsit neh√©zkes feladat megtal√°lni a
                        megfelel≈ë el√©r√©si √∫tat, de a k√∂vetkez≈ë p√©lda ezt szeml√©lteti:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">response = requests.get('http://ec.europa.eu/eurostat/wdds/rest/data/v2.1/json/en/nama_10_gdp?geo=EU28&precision=1&na_item=B1GQ&unit=CP_MEUR&time=2018&time=2019')

response # output: &#60;Response [200]></code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Mivel a fenti esetben a kapott f√°jl egy JSON √°llom√°ny, a requests k√∂nyvt√°r
                        rendelkezik egy met√≥dussal, amely lehet≈ëv√© teszi azt, hogy el√©rj√ºk a v√°lasz adatj√°t, mint egy Python
                        dict. Ez
                        tartalmazni fogja a metaadatot √©s egy list√°nyi adatelemet:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">response.json()

""" 
output: 

{'version': '2.0',
 'label': 'GDP and main components (output, expenditure and income)',
 'href': 'http://ec.europa.eu/eurostat/wdds/rest/data/v2.1/json/en/nama_10_gdp?geo=EU28&precision=1&na_item=B1GQ&unit=CP_MEUR&time=2018&time=2019',
 'source': 'Eurostat',
 'updated': '2020-07-01',
 'extension': {'datasetId': 'nama_10_gdp',
  'lang': 'EN',
  'description': None,
  'subTitle': None},
 'class': 'dataset',
 'value': {'0': 15915732.9, '1': 16452065.5},
 'dimension': {'unit': {'label': 'unit',
   'category': {'index': {'CP_MEUR': 0},
    'label': {'CP_MEUR': 'Current prices, million euro'}}},
  'na_item': {'label': 'na_item',
   'category': {'index': {'B1GQ': 0},
    'label': {'B1GQ': 'Gross domestic product at market prices'}}},
  'geo': {'label': 'geo',
   'category': {'index': {'EU28': 0},
    'label': {'EU28': 'European Union - 28 countries (2013-2020)'}}},
  'time': {'label': 'time',
   'category': {'index': {'2018': 0, '2019': 1},
    'label': {'2018': '2018', '2019': '2019'}}}},
 'id': ['unit', 'na_item', 'geo', 'time'],
 'size': [1, 1, 1, 2]}
"""</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal pb-3">Az el≈ëz≈ë p√©ld√°kban mindig 'nyers' m√≥don weboldalakat, vagy JSON
                        √°llom√°nyokat
                        √©rt√ºnk
                        el a requests k√∂nyvt√°r seg√≠ts√©g√©vel, a k√∂vetkez≈ëkben n√©zz√ºk meg azt is mik√©nt tudjuk API-kkal val√≥
                        kommunik√°ci√≥kra is haszn√°lni. Az API-kkal val√≥ kommunik√°ci√≥ra akkor van sz√ºks√©g, amennyiben az adat amit
                        keres√ºnk nem tal√°lhat√≥ meg a web-en, de tal√°n el√©rhetj√ºk egy API-n kereszt√ºl. Ez mag√°ba foglalja azt,
                        hogy egy
                        lek√©r√©st int√©z√ºnk a megfelel≈ë szerverhez, hogy el√©rj√ºk az adatot egy fix vagy √°ltalunk v√°laszthat√≥
                        form√°tumban.</p>
                    <p class="text-lg font-normal pb-3">A web-es API-k √°ltal leggyakrabban haszn√°lt form√°tumok a JSON √©s az XML.
                        Sz√°munkra
                        az els≈ë lesz a fontos, kor√°bban eml√≠tett okokb√≥l kifoly√≥lag. Sz√°mos megk√∂zel√≠t√©s van arra, hogy arra,
                        hogy
                        l√©trehozzunk egy web-es API-t, √©s egy j√≥ ideig egy kis h√°bor√∫hoz hasonl√≠t√≥ harc volt a k√ºl√∂nb√∂z≈ë domin√°ns
                        architekt√∫r√°k k√∂z√∂tt: REST vs. XML-RPC vs. SOAP. Manaps√°g, a REST t≈±nik diadalmasnak, ami egy nagyon j√≥
                        dolog,
                        hiszen egyszer≈±bb a haszn√°lata √©s k√∂nnyedebb, mint b√°rmelyik alternat√≠va, szerencs√©re a szabv√°nyos√≠t√°snak
                        k√∂sz√∂nhet≈ëen nagyon val√≥sz√≠n≈±v√© teszi azt, hogy felismered √©s gyorsan √°tszoksz b√°rmely √∫j API-hoz, amivel
                        b√°rmikor is tal√°lkozol. Ide√°lis, ak√°r a k√≥dodat is teljesen √∫jrahaszn√°lhatod.</p>
                    <p class="text-lg font-normal pb-3">A legt√∂bb t√°voli adat el√©r√©s √©s manipul√°l√°s n√©gy m≈±veletben
                        √∂sszefoglalhat√≥:
                        hozzunk l√©tre valamit, k√©rj√ºnk le valamit, friss√≠ts√ºnk valamit √©s v√©g√ºl t√∂r√∂lj√ºnk valamit (CRUD). A HTTP
                        biztos√≠tja
                        ezen alapvet≈ë m≈±veleteket a POST, GET, PUT, DELETE utas√≠t√°sokkal, √©s a REST pontosan ezen utas√≠t√°sok
                        absztrakci√≥j√°ra √©p√ºl, mindenk√∂zben √©p√≠tve az URI-ra.</p>
                    <p class="text-lg font-normal pb-3">Sz√°mos vita van arr√≥l, hogy mi a megfelel≈ë √©s mi a nem megfelel≈ë RESTful
                        interf√©sz,
                        de l√©nyeg√©ben az URI-nak <em>(pl. http://example.com/api/items/2)</em>, tartalmaznia kellene mindazon
                        inform√°ci√≥kat
                        amelyek sz√ºks√©gesek ahhoz, hogy v√©gbemehessen egy eml√≠tett CRUD m≈±velet. √ögy lehet elk√©pzelni az eg√©sz
                        munk√°t,
                        hogy az URI a virtu√°lis el√©r√©se az adatnak, a CRUD pedig a m≈±veleteket jelenti, amelyeket az adattal
                        k√©pesek
                        vagyunk elv√©gezni.</p>
                    <p class="text-lg font-normal">N√©zz√ºk meg teh√°t hogyan tudjuk a requests k√∂nyvt√°rat RESTful web API el√©r√©s√©re
                        haszn√°lni. Ebben az esetben m√°r √∂sszetetebb folyamatokra haszn√°ljuk a k√∂nyvt√°r, argumentumokkal ell√°tott
                        URL-t
                        pr√≥b√°lunk el√©rni vele. Az al√°bbi k√≥dr√©szlet szeml√©lteti azt, hogy milyen f√ºggv√©nyt kellene √≠rni ahhoz,
                        hogy az
                        OECD adatb√°zisb√≥l az API-j√ºk√∂n kereszt√ºl √©rj√ºnk el adatokat:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">OECD_ROOT_URL = 'http://stats.oecd.org/sdmx-json/data'

def make_OECD_request(dsname, dimensions, params=None, root_dir=OECD_ROOT_URL):
	""" L√©trehozza az URL-t, majd visszat√©r√≠ti a GET request v√°lasz√°t """

	if not params:
		params = {}
	# az√©rt van a fentire sz√ºks√©g, mert nem szabad alap√©rtelmezett param√©terekk√©nt v√°ltoztathat√≥ Python √©rt√©keket megadni

	dim_args = ['+'.join(d) for d in dimensions]
	dim_str = '.'.join(dim_args)
	# a n√©gy dimenzi√≥s argumentumok list√°ja, argumentumokon bel√ºl +-al √∂sszef≈±zve, az argumentumok sor√°t pedig .-al √∂sszef≈±zve
	# pl. AUS+AUT.GDP+B1_G3.CUR+VOBARSA.Q

	url = root_dir + '/' + dsname + '/' + dim_str + '/all'

	print('Requesting OECD URL: {}'.format(url))

	return requests.get(url, params=params)
	# a requests GET met√≥dusa ak√°r egy param√©ter dict-et is k√©pes bevenni, hogy az URL-t tov√°bb alak√≠tsa, egy URL query string-g√©
	# pl. startTime=2009-Q2&endTime=2011-Q4

response = make_OECD_request('QNA', (('USA', 'AUS'), ('GDP', 'B1_GE'), ('CUR', 'VOBARSA'), ('Q')), {'startTime': '2009-Q1', 'endTime': '2010-Q1'})
# output: Requesting OECD URL: http://stats.oecd.org/sdmx-json/data/QNA/USA+AUS.GDP+B1_GE.CUR+VOBARSA.Q/all</code><button
    id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Miut√°n l√©trehoztuk a linket √©s a lek√©rdez√©s ut√°n megkaptuk r√° a v√°laszt is,
                        amely
                        tartalmazza a gazdas√°gi adatait az AE√Å-nak √©s Ausztr√°li√°nak 2009 els≈ë negyed√©v√©t≈ël 2010 els≈ë negyed√©v√©ig,
                        n√©zz√ºk meg, hogy kaptunk-e valami inform√°ci√≥t vissza, √©s ha igen, akkor mit:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">if response.status_code == 200:
	json = response.json()
	json.keys()
    # output: dict_keys(['header', 'dataSets', 'structure'])</code><button id="copy"
        class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal pb-3">A kapott JSON adat SDMX form√°tumban van, amely statisztikai adatok
                        k√∂zl√©s√©re
                        volt tervezve. Nem a legintu√≠tivabb form√°tum, de ez gyakran megt√∂rt√©nik az adatokkal val√≥ munka sor√°n.
                        Szerencs√©re, majd a Pandas k√∂nyvt√°r orvosolja a probl√©m√°t, hiszen rendelkezik a pandaSDMX-el.</p>
                    <p class="text-lg font-normal pb-3">Van n√©h√°ny nemzeti statisztika, amely hasznos lesz sz√°munkra mik√∂zben a
                        v√©gs≈ë
                        projektet k√©sz√≠ts√ºk el. A lakoss√°g m√©rete, a 3 karakteres nemzetk√∂zi azononos√≠t√≥-k√≥dok √©s m√©g sok m√°s
                        egy√©b is
                        potenci√°lis hasznoss√° v√°lhat, amikor valamilyen nemzetk√∂zi d√≠jat √©s annak eloszt√°s√°t vizsg√°ljuk √©s
                        pr√≥b√°ljuk
                        megjelen√≠teni.</p>
                    <p class="text-lg font-normal">A <em>REST countries</em> egy nagyon k√∂nnyen kezelhet≈ë webes forr√°s, amely
                        k√ºl√∂nb√∂z≈ë nemzetk√∂zi statisztik√°kat tartalmaz. Haszn√°ljuk fel n√©mi adat lek√©r√©s√©re:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">REST_EU_ROOT_URL = 'http://restcountries.eu/rest/v1'
    
def REST_country_request(field='all', name=None, params=None):
    headers = { 'User-Agent' : 'Mozilla/5.0' }
    # legt√∂bbsz√∂r j√≥ √∂tlet az, ha meghat√°rozzunk egy valid 'User-Agent'-et a request fejl√©c√©ben,
    # annak √©rdek√©ben tessz√ºk ezt, hogy elker√ºljek azt az esetet, amikor nem lehet az adatot el√©rni meghat√°rozott fejl√©c n√©lk√ºl
    # a fejl√©cek nem m√°sok, mint kieg√©sz√≠t≈ë inform√°ci√≥k a HTTP csatlakoz√°s sor√°n
    # ebben az esetben az 'User-Agent' l√©nyeg√©ben inform√°ci√≥t szolg√°ltat az API-nak arr√≥l, hogy milyen rendszer k√©ri le az inform√°ci√≥t,
    # azonos√≠tva ez √°ltal
    	
    if not params:
    	params = {}

    if field == 'all':
            return requests.get(REST_EU_ROOT_URL + '/all')
        
    url = '{}/{}/{}'.format(REST_EU_ROOT_URL, field, name)
    print('Requesting URL: {}'.format(url))
    response = requests.get(url, params=params, headers=headers)
    # egy REST countries URL √≠gy n√©z ki: https://restcountries.eu/rest/v1/&#60;field>/&#60;name>?&#60;params>

    if not response.status_code == 200:
        raise Exception('Request failed with status code {}'.format(response.status_code))
    # miel≈ëtt visszat√©r√≠ten√©nk a v√°laszt, megn√©zz√ºk √©rv√©nyes-e

    return response</code><button id="copy"
        class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Miut√°n megvan a f√ºggv√©ny, amellyel az OECD p√©ld√°hoz hasonl√≥an lek√©r√©seket
                        int√©zhet√ºnk az API-hoz, k√©rj√ºk le az √∂sszes orsz√°got, amelynek doll√°r a p√©nzneme:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">response = REST_country_request('currency', 'usd')
response.json()</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Tov√°bbi gyakorl√°sk√©ppen mivel a REST orsz√°gok adatsorja el√©g kicsi, csin√°ljunk
                        bel≈ële egy helyi m√°solatot √©s t√°roljuk MongoDB-n a nobel-prize adatb√°zisban, el√©rve az adatb√°zist a
                        kor√°bban
                        l√©trehozott f√ºggv√©ny√ºnket a <em>get_mongo_database</em>-t.</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">from pymongo import MongoClient

def get_mongo_database(db_name, host='localhost', port=27017, username=None, password=None):
    if username and password:
        mongo_uri = 'mongodb://{}:{}@{}/{}'.format(username, password, host, db_name)
        conn = MongoClient(mongo_uri)
    else:
        conn = MongoClient(host, port)

    return conn[db_name]

db = get_mongo_database('nobel_prize')
col = db['country_data'] # egy collection az orsz√°gadatokkal

# kor√°bbi l√©p√©sben lek√©rt√ºk √©s t√°roltuk a response v√°ltoz√≥ba a REST country request-et

col.insert_many(response.json())
# beillesztj√ºk a kapott JSON √°llom√°nyt a collectionbe</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Miut√°n elhelyezt√ºk az orsz√°gos adatokat a MongoDB collection-√ºkbe, k√©rj√ºk le
                        az
                        adatb√°zisb√≥l az √∂sszes olyan orsz√°got, ahol doll√°r a p√©nznem:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">res = col.find({'currencies': {'$in': ['USD']}})
list(res)</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal pb-3">Na m√°r most, miut√°n meg√≠rt√ºnk n√©h√°ny saj√°t API kezel≈ët, n√©zz√ºnk meg
                        n√©h√°ny
                        √∂sszetetebb API kezel√©s√©re dedik√°lt k√∂nyvt√°rat, amelyek a nagy API-kat k√∂nnyen kezelhet≈ëv√© teszik. A
                        requests
                        k√∂nyvt√°r majdnem minden API-val k√©pes egy√ºtt m≈±k√∂dni, √©s legt√∂bbsz√∂r csak olyan apr√≥ f√ºggv√©nyekre van
                        sz√ºks√©g,
                        mint amit a kor√°bbi p√©ld√°kban is l√°thattunk. De, ahogy az API-k is elkezdnek hiteles√≠t√©seket haszn√°lni,
                        √©s az
                        adatstrukt√∫ra is egyre bonyolultabb√° v√°lik, n√©ha j√≥l j√∂n egy kieg√©sz√≠t≈ë k√∂nyvt√°r, amely megk√∂nny√≠ti a
                        folyamatot.</p>
                    <p class="text-lg font-normal pb-3">Egyre gyakoribb manaps√°g, hogy az adatsorokat a felh≈ëben t√°rolj√°k.
                        P√©ldak√©ppen,
                        megt√∂rt√©nhet az, hogy az az adat, amit √°br√°zolni kellene Google Spreadsheet-ben tal√°lhat√≥, ami egy
                        csoporton
                        bel√ºl van megosztva. A legjobb megold√°s az, ha valamilyen m√≥don megszerezz√ºk az adatot √©s Pandas-el
                        vizsg√°ljuk
                        meg, de egy√©b lehet≈ës√©gek is rendelkez√©s√ºnkre √°llnak, mint pl. a Gspread k√∂nyvt√°r.</p>
                    <p class="text-lg font-normal pb-3">A Gspread k√∂nyvt√°r, az egyik legelterjedtebb, ami a Google
                        Spreadsheet-ekhez
                        val√≥
                        hozz√°f√©r√©st biztos√≠tja √©s viszonylag k√∂nnyedd√© teszi vel√ºk a munk√°t. Sz√ºks√©g√ºnk lesz OAuth 2.0
                        credentials-ra
                        ahhoz, hogy haszn√°lhassuk az API-t.R√∂viden √©s legegyszer≈±bb form√°j√°ban az OAuth egy olyan hozz√°f√©r√©si
                        standard, amely √∫gy teszi lehet≈ëv√© egy felhaszn√°l√≥ weboldalaknak vagy applik√°ci√≥knak az adatokhoz val√≥
                        hozz√°f√©r√©st, hogy nem adja meg mindek√∂zben sz√°mukra az el√©r√©si jelsz√≥t. A Google-nek egy hasznos kis
                        √∫tmutat√≥ja van ahhoz, hogy megszerezz√ºk a saj√°t priv√°t kulcsunkat az adatokhoz val√≥ hozz√°f√©r√©shez: <a
                            href="https://developers.google.com/identity/protocols/oauth2/service-account"
                            class="group text-lg pb-8 text-gray-600
                    hover:text-blue-800 opacity-75 hover:opacity-100 transition duration-100 ease-in">√∫tmutat√≥ <span class="inline-block font-normal transform group-hover:translate-x-1 transition duration-100
                    ease-in">‚Üó</span></a> .</p>
                    <p class="text-lg font-normal">Az √∫tmutat√≥ l√©p√©seit k√∂vetve, meg is kaptunk egy JSON √°llom√°nyt amely
                        tartalmazza a
                        priv√°t kulcsot. Ahhoz, hogy haszn√°lhassuk a k√∂nyvt√°rt telep√≠ten√ºnk kell, illet≈ëleg a legfrisebb OAuth2
                        klienssel is rendelkezn√ºnk kell:</p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="shell">$ pip install gspread
$ pip install --upgrade oauth2client</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">A megszerzett credentials-al b√°rmely spreadsheet-et k√©pes kellene legy√©l
                        el√©rni
                        mind√∂ssze n√©h√°ny sorral. A k√∂vetkez≈ë p√©lda azt szeml√©lteti hogyan lehet bet√∂lteni egy ilyen
                        spreadsheet-et:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">import json
import gspread
from google.oauth2.service_account import Credentials

scope = ['https://spreadsheets.google.com/feeds',
		'https://www.googleapis.com/auth/drive']

# fontos megjegyezni azt, hogy mindk√©t fenti scope-ban tal√°lhat√≥ API-t enged√©lyezni kellett a projekt service account-j√°ra a Google APIs-b√≥l
# teh√°t a Google Sheets API-t, √©s a Google Drive API-t

# a credentials JSON √°llom√°ny a Google Services-t≈ël van
credentials = Credentials.from_service_account_file('../key/pyjsviz-282509-6cfa775f3db7.json', scopes=scope)

# ezzel hiteles√≠tj√ºk a hozz√°f√©r√©s√ºnket a Google Spreadsheet-ekhez
gc = gspread.authorize(credentials)

# megnyitjuk az URL-vel hivatkozott Google Spreadsheet-et
ss =
gc.open_by_url('https://docs.google.com/spreadsheets/d/1kHCEWY-d9HXlWrft9jjRQ2xf6WHQlmwyrXel6wjxkW8/edit#gid=0')</code><button
    id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Miut√°n, sikeresen el√©rt√ºk a Google Spreadsheet-et Pythonb√≥l, l√©trehozva a
                        service
                        account-ot, let√∂ltve annak hiteles√≠t≈ë JSON √°llom√°ny√°t az applik√°ci√≥nk hiteles√≠t√©s√©hez, valamint
                        enged√©lyezve a
                        sz√ºks√©ges API-kat, ak√°r meg is n√©zhetj√ºk, hogy az el√©rt Spreadsheet milyen munkalapokkal rendelkezik:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">ss.worksheets()</code><button id="copy"
                            class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Ez ut√°n ak√°r egy munkalapot ki is v√°laszthatunk, hogy el√©rj√ºk a cell√°iban
                        tal√°lhat√≥
                        adatokat:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">ws = ss.worksheet('bugs')</code><button id="copy"
                            class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">A k√∂vetkez≈ëkben ak√°r t√©nyleges le is k√©rhetj√ºk az adatokat, lek√©rve a teljes
                        els≈ë
                        oszlop c√©ll√°it:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">ws.col_values(1)</code><button id="copy"
                            class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Ak√°r gspread-b≈ël k√∂zvetlen√ºl √©s k√©pesek vagyunk √°br√°zolni a munkalapon
                        tal√°lhat√≥
                        inform√°ci√≥kat, de m√©gis jobb szakosodott k√∂nyvt√°rakat haszn√°lni erre, mint a Pandas-t. A gspread
                        get_all_records met√≥dus√°t haszn√°lva k√©pesek vagyunk az √∂sszes oszlopot lek√©rni, amivel majd k√©pesek
                        lesz√ºnk a
                        Pandas DataFrame-t inicializ√°lni:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">import pandas as pd

df = pd.DataFrame(ws.get_all_records())</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal pb-3">√Ått√©rve a k√∂vetkez≈ë k√∂nyvt√°rra, amelyet bonyolultabb API-kkal val√≥
                        munk√°kra
                        haszn√°lhatunk, besz√©ln√ºnk kell a Tweepy-r≈ël. A k√∂z√∂ss√©gi m√©dia f√©nykor√°ban rengeteg adat gener√°l√≥dik
                        ezeken a
                        platformokon. A Twitter platform 'broadcast' h√°l√≥zata egy nagyon gazdag adatforr√°s vizualiz√°ci√≥khoz √©s
                        annak
                        API-ja ak√°r sz≈±r√©sen √°tesett hozz√°f√©r√©st is biztos√≠t a tweet-ekhez, felhaszn√°l√≥kk√©nt, hashtagenk√©nt,
                        d√°tumonk√©nt, stb.
                    </p>
                    <p class="text-lg font-normal pb-3">A Tweepy egy k√∂nnyen haszn√°lhat√≥ Twitter k√∂nyvt√°r √©rtelemszer≈±en, amely
                        sz√°mos
                        hasznos feature-t biztos√≠t, mint p√©ld√°ul a StreamListener class-t, hogy √©l≈ëben streamelhess√ºk a Twitter
                        friss√≠t√©seket. Ahhoz, hogy haszn√°lhassuk el≈ësz√∂ris sz√ºks√©g√ºnk van egy Twitter access token-re, amely
                        el√©rhet≈ë
                        a k√∂vetkez≈ë √∫tmutat√≥ haszn√°lat√°val: <a
                            href="https://developer.twitter.com/en/docs/basics/authentication/oauth-1-0a/obtaining-user-access-tokens"
                            class="group text-lg pb-8 text-gray-600
                    hover:text-blue-800 opacity-75 hover:opacity-100 transition duration-100 ease-in">√∫tmutat√≥ <span class="inline-block font-normal transform group-hover:translate-x-1 transition duration-100
                    ease-in">‚Üó</span></a> .
                    </p>
                    <p class="text-lg font-normal pb-3">A k√∂nyvt√°r m≈±k√∂d√©s√©t nem tudom megvizsg√°lni, nem rendelkezek Twitter-rel,
                        de
                        az √∫tmutat√≥ja a k√∂vetkez≈ë linken el√©rhet≈ë: <a href="http://docs.tweepy.org/en/latest/"
                            class="group text-lg pb-8 text-gray-600
                    hover:text-blue-800 opacity-75 hover:opacity-100 transition duration-100 ease-in">√∫tmutat√≥ <span class="inline-block font-normal transform group-hover:translate-x-1 transition duration-100
                    ease-in">‚Üó</span></a> .
                    </p>
                    <p class="text-lg font-normal pb-3">K√∂vetkez≈ëkben besz√©ln√ºnk kell a 'Scrape-r≈ël'. Ezzel a sz√≥val jel≈ël√ºnk egy
                        folyamatot, amikor is √∫gy szerz√ºnk meg valamilyen adatot a netr≈ël, hogy val√≥j√°ban azt sosem tervezt√©k meg
                        erre. Alapvet≈ëen, ez a folyamat az√©rt lehet neh√©zkes sok esetben, mert ebben az esetben nek√ºnk kell
                        eld√∂nten√ºnk azt, hogy mit √©rdemes kihagyni, mi n√©lk√ºl√∂zhetetlen, teh√°t megkell tal√°lni az egyens√∫lyt.
                        Olyan
                        proced√∫r√°k l√©trehoz√°sa, amelyek pontosan a sz√ºks√©ges adatot szerzik meg, annyira tiszt√°n, amennyire csak
                        lehet, olyan weboldalakr√≥l, amelyek sok esetben k√∂vethetetlenek egy k√©pess√©g m√°r √∂nmag√°ban is.
                    </p>
                    <p class="text-lg font-normal pb-3">Az interneten egyar√°nt tal√°lhat√≥ olyan adat, amely √∂nmag√°ban tiszta √©s
                        haszn√°lhat√≥, de m√©gis a legt√∂bbje √∂nmag√°ban fogyaszthatatlan √©s felhaszn√°lhatatlan eredeti form√°j√°ban.
                        Ez√©rt
                        sz√ºks√©ges olyan kezel√©si folyamatokat l√©trehozni, amely az ember √°ltal m√©g √©ppen √©rthet≈ë
                        adat-halmazokb√≥l,
                        haszn√°lhat√≥ adatsorokat hoznak l√©tre, amelyeket k√©s≈ëbb √°br√°zolhatunk.
                    </p>
                    <p class="text-lg font-normal pb-3">Ebben a r√©szben egy ilyen probl√©m√°t fogunk megoldani, mik√∂zben
                        megpr√≥b√°ljuk megszerezni a Nobel-gy≈ëztesek list√°j√°t. A probl√©ma megold√°s√°hoz felhaszn√°ljuk a
                        BeautifulSoup k√∂nyvt√°rat, de az ezt k√∂vet≈ë fejezetben egy √∫jabb Scrapel√©sre szakosodott k√∂nyvt√°rat fogunk
                        megvizsg√°lni, n√©vszerint a Scrapy-t.
                    </p>
                    <p class="text-lg font-normal pb-3">Alapvet≈ëen, a Pythonnak k√©t lightweight scrapel√©sre szakosodott k√∂nyvt√°ra
                        van: BeautifulSoup √©s lxml. Az els≈ëdleges szintaxisuk, ami a kiv√°laszt√°st illeti k√ºl√∂nb√∂zik, de, hogy
                        j√∂bben √∂sszezavarja a dolgok √°ll√°s√°t, mindk√©t k√∂nyvt√°r k√©pes egym√°s √©rtelmez≈ëj√©t (parser) haszn√°lni. Az
                        √°ltal√°nos megegyez√©s √∫gy √°ll, hogy az lxml √©rtelmez≈ëje gyorsabb, azonban a BeautifulSoup sokkal jobban
                        kezeli a rosszul meg√≠rt HTML-t. Ugyanakkor, az lxml kiv√°laszt√°sa, amely xpath-re alapszik, sokkal jobban
                        hasonl√≠t a jQuery √©s CSS
                        √°ltal haszn√°lt selectorokhoz, √≠gy webfejleszt≈ëk sz√°m√°ra az tal√°n egy intu√≠tivabb eszk√∂z lenne.
                    </p>
                    <p class="text-lg font-normal">El≈ësz√∂ris, n√©zz√ºk meg a BeautifulSoup k√∂nyvt√°rat. Amelyet k√∂nnyed√©n
                        feltelep√≠thet√ºnk a pip seg√≠ts√©g√©vel:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="shell">$ pip install beautifulsoup4</code><button id="copy"
                            class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal pb-3">Minden rendelkez√©s√ºnkre √°ll, hogy megszerezz√º√∂k magunknak a neveiket,
                        nyer√©s√ºknek √©v√©t, kategori√°jukat √©s v√©g√ºl nemzetis√©g√ºket a Nobel-gy≈ëzteseknek, mind√∂ssze BeautifulSoup-ot
                        √©s requests-et haszn√°lva. Kiindul√°sk√©ppen haszn√°ljuk a Wikip√©dia Nobel-d√≠j oldal√°t, ahol egy t√°bl√°zatban
                        felvannak sorolva az eddigi gy≈ëztesek.
                    </p>
                    <figure>
                        <img class="mx-auto"
                            src="../img/fig2.png"
                            alt="√Åbra 2">
                        <figcaption class="text-center pt-3 pb-4 text-md">2. √Åbra: Nobel-gy≈ëztesek list√°ja</figcaption>
                    </figure>
                    <p class="text-lg font-normal pb-3">Sz√ºks√©g√ºnk van valamilyen HTML kezel≈ëre ahhoz, hogy megvizsg√°ljuk mib≈ël is
                        √©p√ºl fel a weboldal, term√©szetesen a Chrome DevTools t√∂k√©letes a feladatra. Alapvet≈ëen, azt sz√ºks√©ges
                        megfigyeln√ºnk, hogy hogyan √©p√ºl fel a weboldal, hogyan tudjuk kiv√°lasztani a √©rdekelt adatokat, mik√∂zben
                        elker√ºlj√∂k a felesleges tartalmi elemeket. A jelenesetben, a t√°bl√°zat az √©rdekes a sz√°munkra. Az eg√©sz
                        scrapel√©si folyamat arra alapszik, hogy k√©pesek vagyunk-e hat√©kony megalkotni a selectorunkat, amellyel
                        pontosan azt az adatot √©rj√ºk el a weboldalon, amire sz√ºks√©g√ºnk van.
                    </p>
                    <p class="text-lg font-normal pb-3">Az el≈ëbb eml√≠tettem, hogy a lxml kiv√°laszt√≥ja xpath-re alapszik, amit
                        nat√≠van biztos√≠t sz√°munkra a Chrome DevTools. Ahhoz, hogy el√©rj√ºk egy HTML elem xpath-j√©t, az Elements
                        tab-on bel√ºl az elemre jobb-klikkelve √©s a Copy kontext-men√ºpontot kiv√°lasztva el√©rhetj√ºk az xpath
                        el√©r√©s√©t, ami valahogy √≠gy n√©z ki: <em>//*[@id="mw-content-text"]/div/table[1]</em>.
                    </p>
                    <figure>
                        <img class="mx-auto"
                            src="../img/fig3.png"
                            alt="√Åbra 3">
                        <figcaption class="text-center pt-3 pb-4 text-md">3. √Åbra: Nobel-gy≈ëztesek list√°j√°nak xpath-je
                        </figcaption>
                    </figure>
                    <p class="text-lg font-normal">Az els≈ë dolog, amit tenn√ºnk kell miel≈ëtt t√©nyleges scrapelhetn√©nk a
                        weboldal az, hogy √©rtelmezz√ºk a weboldalt a BeautifulSoup seg√≠ts√©g√©vel, √°talak√≠tva a HTML-t egy faszer≈±
                        elrendez√©sbe, amely a HTML tag-ekb≈ël √°ll (tag tree hierarchy), a k√∂nyvt√°r eset√©ben ezt Soup-nak szok√°s
                        h√≠vni.
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">from bs4 import BeautifulSoup
import requests

BASE_URL = 'https://en.wikipedia.org'
# A Wikip√©dia eset√©ben m√°r fontos, hogy meghat√°rozzuk a 'User-Agent' fejl√©cet, amely alapj√°n azonos√≠tani tudja azt, hogy
# milyen rendszerb≈ël pr√≥b√°ljuk el√©rni a weboldal tartalm√°t
HEADERS = { 'User-Agent': 'Mozilla/5.0' }

def get_Nobel_soup():
	""" Visszat√©r√≠ti az √©rtelmezett tag f√°t a Nobel-d√≠jas weboldalunkr√≥l """
	# egy lek√©r√©st int√©z a Nobel-d√≠jas oldalhoz, be√°ll√≠tva a valid Headers fejl√©cet

	response = requests.get('{}/wiki/List_of_Nobel_laureates'.format(BASE_URL), headers=HEADERS)

	if not response.status_code == 200:
		raise Exception('Something wrong happened with status code: {}'.format(response.status_code))

	return BeautifulSoup(response.content, 'lxml')
	# ebben az esetben m√°r megmutatkozik az, hogy a k√©t k√∂nyvt√°r k√©pes egym√°s parserj√©t haszn√°lni,
        # a m√°sodik argumentum a parsert lxml-re √°ll√≠tja</code><button id="copy"
            class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Miut√°n megvan a soup, n√©zz√ºk meg hogyan tudjuk megtal√°lni a c√©ltageket,
                        amelyre sz√ºks√©g√ºnk van az √©rdemi adat kinyer√©s√©hez. A BeautifulSoup n√©h√°ny m√≥dot biztos√≠t arra, hogy
                        tageket v√°lasszunk ki egy m√°r parselt soup-b√≥l. Megfigyelhetj√ºk az Chrome DevTools Elements tabj√©ben azt,
                        hogy a t√°bl√°zatuknak k√©t fontos classja van, n√©vszerint a <em>wikitable</em> √©s <em>sortable</em>.
                        Haszn√°lhatjuk a k√∂nyvt√°r find met√≥dus√°dat, hogy megtal√°ljuk az els≈ë table taget az eml√≠tett classokkal. A
                        met√≥dus beveszi a tag nev√©t, mint els≈ë argumentum, √©s egy dict-et a clasokkal, id-kkal √©s m√°s egy√©b
                        azonos√≠t√≥kkal. A k√∂vetkez≈ë k√≥dr√©szlet ezt szeml√©lteti:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">soup = get_Nobel_soup()

soup.find('table', {'class': 'wikitable sortable'})</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal pb-3">Hab√°r, a fenti p√©lda ebben az esetben m≈±k√∂dik, m√©gsem robusztus a
                        megold√°s,
                        el√©g, ha csak a classok sorrendj√©t megv√°ltoztatjuk a dict-en bel√ºl, √©s m√°ris m√°s eredm√©nyt fogunk kapni.
                        Ez√©rt mondhat√≥ az el, hogy a BeautifulSoup selectorja al√∫lmarad az lxml selectorj√°t√≥l.
                    </p>
                    <p class="text-lg font-normal">Felhaszn√°lva a soup select met√≥dus√°t, amely csak akkor √°ll rendelkez√©s√ºnkre, ha
                        az lxml-t hat√°roztuk meg annak parserjek√©nt, k√©pes vagy meghat√°rozni egy HTML elemet a CSS classait
                        felhaszn√°lva. Ezeket a CSS selectorokat felhaszn√°lva az xpath szintaxtisba fogja √°talak√≠tani az lxml
                        teljesen bels≈ëleg, b√°rmif√©le t√∂bbletl√©p√©s n√©lk√ºl. Tekints√ºk meg a k√∂vetkez≈ë k√≥dr√©szletet:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">soup.select('table.sortable.wikitable')
# ugyanazt az eredm√©nyt kapjunk, mint az el≈ëbb, azonban ebben az esetben nem okoz j√∂v≈ëbeli gondot az, ha a sorrend
megv√°ltozik</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Az el≈ëbbi esetben egy list√°nyi elemet t√©r√≠tett vissza a select met√≥dus,
                        amennyiben egyetlen elemet szeretn√©k kiv√°lasztani haszn√°lhatjuk a select_one met√≥dust:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">table = soup.select_one('table.sortable.wikitable')
table.select('th')
# kiv√°lasztja az √∂sszes table-header elemet a t√°bl√°zaton bel√ºl</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">√ârdekess√©g:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">table.select('th')
# a k√©t jel√∂l√©s ekvivalens
table('th')</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal pb-3">Az lxml parserj√©vel a BeautifulSoup k√∂nyvt√°r sz√°mos lehet≈ës√©get biztos√≠t
                        arra,
                        hogy sz≈±rj√ºk a keresett tageket, az el≈ëbb haszn√°lt egyszer≈± string n√©v mellett, ak√°r haszn√°lhatunk
                        regExp-et keres√©skor, egy list√°nyi tag nevet is, de egy√©b m√≥dok is vannak, a k√∂vetkez≈ë √∫tmutat√≥
                        tartalmazza a lehets√©ges sz≈±r√©si lehet≈ës√©geket: <a
                            href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#kinds-of-filters"
                            class="group text-lg pb-8 text-gray-600
                    hover:text-blue-800 opacity-75 hover:opacity-100 transition duration-100 ease-in">√∫tmutat√≥ <span class="inline-block font-normal transform group-hover:translate-x-1 transition duration-100
                    ease-in">‚Üó</span></a> .
                    </p>
                    <p class="text-lg font-normal pb-3">Miut√°n megtanultuk azt hogyan tudjuk kiv√°lasztani a Wikip√©dia
                        t√°bl√°zatunkat √©s
                        felfegyverkezt√ºnk az lxml kiv√°laszt√°si met√≥dusaival, n√©zz√ºk meg hogyan tudunk √∂sszehozni n√©h√°ny
                        kiv√°laszt√°si mint√°t (selection pattern), hogy megszerezz√ºk azt az adatot, amire sz√ºks√©g√ºnk van.
                    </p>
                    <p class="text-lg font-normal">K√∂vetkez≈ë l√©p√©shez, megkell vizsg√°lnunk azt, hogy hogyan is van megjelen√≠tve az
                        adat a HTML strukt√∫r√°ban. Minden egyes gy≈ëztes egy <em>&#60;td></em> elem, amelyen bel√ºl egy hyperlink a
                        gy≈ëztes Wikip√©dia √©letrajzi oldal√°ra mutat, egy√©nek eset√©ben legal√°bbis. Alapvet≈ëen, teh√°t nem m√°st
                        kellene csin√°lnunk, mint v√©gigmenni a sorokon, illet≈ëleg oszlopokon, amik a kateg√≥ri√°t jel≈ëlik, lementve
                        az √©vet, illet≈ëleg a gy≈ëztesek nev√©t. Tekints√ºk meg a k√∂vetkez≈ë k√≥dr√©szletet:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">def get_column_titles(table):
	""" Megszerzi a Nobel kateg√≥ri√°kat a table-headerekb≈ël """

	cols = []

	# [1:]-vel kihagyjuk az els≈ë table-headert, mert az ugye csak a 'year'-t tartalmazza
	for th in table.select_one('tr').select('th')[1:]:
		link = th.select_one('a')
		# t√°rolja a kateg√≥ria nev√©t √©s b√°rmely Wikip√©dia linket, amit tartalmaz

		if link:
			cols.append({'name': link.text, 'href':link.attrs['href']})
		else:
			cols.append({'name': th.text, 'href': None})

    return cols</code><button id="copy"
        class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Ak√°r meg is bizonyosodhatunk arr√≥l, hogy a f√ºggv√©ny val√≥ban j√≥l m≈±k√∂dik:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">print(get_column_titles(table))</code><button id="copy"
                            class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal pb-3">Teh√°t, m√°r alapvet≈ëen ismerj√ºk a t√°bl√°zat fel√©p√≠t√©s√©t, √©s ki is nyert√ºk
                        bel≈ële
                        az els≈ë relev√°ns adatot.
                    </p>
                    <figure>
                        <img class="mx-auto"
                            src="../img/fig4.png"
                            alt="√Åbra 4">
                        <figcaption class="text-center pt-3 pb-4 text-md">4. √Åbra: Nobel-gy≈ëztesek list√°j√°nak fel√©p√≠t√©se
                        </figcaption>
                    </figure>
                    <p class="text-lg font-normal">Azt√°n √≠runk egy f√ºggv√©nyt, amely a gy≈ëzteseket is kinyeri sz√°munkra:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">def get_Nobel_winners(table):
    cols = get_column_titles(table)

    winners = []

    # kihagyjuk az els≈ë √©s utols√≥ t√°bl√°zat sort, mert az a kateg√≥ri√°kat √©s a 'year'-t tartalmazza
    for row in table.select("tr")[1:-1]:
        year = int(row.select_one("td").text)  # lek√©ri az els≈ë td-t, amiben az √©v van

        for i, td in enumerate(row.select("td")[1:]):
            for winner in td.select("a"):
                href = winner.attrs["href"]
                if not href.startswith("#endnote"):
                    winners.append(
                        {
                            "year": year,
                            "category": cols[i]["name"],
                            "name": winner.text,
                            "link": winner.attrs["href"],
                        }
                    )

    return winners</code><button id="copy"
        class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">√ñsszes√©g√©ben, a fenti f√ºggv√©ny eset√©ben nem m√°st tesz√ºnk, mint el≈ësz√∂ris
                        megszerezz√ºk az √©vet, amelyre az adott sor vonatkozik, majd ezt az oszlopot kihagyva, a t√∂bbin v√©gig
                        iter√°lva lementj√ºk a gy≈ëzteseket. Az iter√°ci√≥ eset√©ben az√©rt haszn√°lunk enumerate-ot, mert √≠gy az indexet
                        is nyomon k√∂vethetj√ºk, ami fontos, a megfelel≈ë kateg√≥ria t√°rs√≠t√°shoz. Az #endnote ellen≈ërz√©shez az√©rt van
                        sz√ºks√©g, mert ilyen tartalm√∫ hyperlinkkel rendelkez≈ë anchor tagek is tal√°lhat√≥k a t√°bl√°zatban. A winner
                        selector rendelkezik egy attrs dict-el, amely egy√©b dolgok mellett tartalmazza az anchor tag linkj√©t is.
                        Ellen≈ër√≠zz√ºk le megfelel≈ëen kisz≈±rte-e a Nobel-gy≈ëzteseket a f√ºggv√©ny:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">print(get_Nobel_winners(table))</code><button id="copy"
                            class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal pb-3">Megvan a Nobel-gy≈ëztesek teljes list√°ja, √©s mindegyik gy≈ëztesnek a
                        Wikip√©dia-√©letrajz√°nak a hyperlinkje. Felhaszn√°ljuk mostm√°r ezeket a linkeket, hogy megszerezz√ºk az
                        egy√©nek √©letrajz√°nak sz√∂veg√©t is, ugyanakkor mivel ez egy hosszadalmas √©s er≈ëforr√°sig√©nyes folyamat, jobb
                        csak egyszer v√©grehajtani, teh√°t a probl√©m√°t √∫gy oldjuk meg, hogy a scrapelend≈ë adatot cache-elj√ºk, amely
                        lehet≈ëv√© teszi, hogy √∫gy hajtsuk rajta v√©gre k√ºl√∂nb√∂z≈ë scrapel√©si kis√©rleteket, m√≠g eljutunk a t√©nyleges
                        megold√°shoz, hogy nem kell visszat√©rn√ºnk √∫jra a Wikip√©dia oldal√°hoz.
                    </p>
                    <p class="text-lg font-normal">Szerencs√©re, ez k√∂zelsem olyan bonyolult folyamat, mint amilyennek t≈±nik,
                        haszn√°lhatjuk erre a requests-cache plugint, a requests-hez, mind√∂ssze n√©h√°ny sorral a legalapabb
                        cachel√©si sz√ºks√©gleteket k√©pes ez kiel√©g√≠teni. Feltelep√≠thetj√ºk a k√∂vetkez≈ë parancssori k√≥ddal:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="shell">$ pip install --upgrade requests-cache</code><button id="copy"
                            class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Itt tekinthet≈ë meg a legegyszer≈±bb form√°ja a requests-cache-nek:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">import requests
import requests_cache

requests_cache.install_cache()
# haszn√°ljuk a tov√°bbiakban a szok√°sos m√≥don a requests k√∂nyvt√°rat ...</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Az install_cache met√≥dus j√≥n√©h√°ny hasznos be√°ll√≠t√°ssal is rendelkezik,
                        amelyekben olyan dolgokat tudunk meghat√°rozni, mint p√©ld√°ul a backend, hol ker√ºlj√∂n t√°rol√°sra a cache,
                        p√©ld√°ul sqlite, memory, mongodb, stb, vagy ak√°r lej√°rati id≈ët is meghat√°rozhatunk neki m√°sodpercekben,
                        stb. A k√∂vetkez≈ë be√°ll√≠t√°s p√©ld√°ul elind√≠tja a cachel√©st sqlite-ra mentve, 2 √≥r√°s lej√°rati id≈ëvel:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python"># elind√≠t egy nobel_pages nev≈± cachet, sqlite backend-el √©s 2 √≥r√°s lej√°rati id≈ëvel
requests_cache.install_cache('nobel_pages', backend='sqlite', expire_after=7200)</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Teh√°t, m√°r minden a hely√©n van ahhoz, hogy megszerezz√ºk a gy≈ëztesek
                        nemzetis√©g√©t, jelenleg csak az els≈ë √∂tven√©t, pr√≥b√°lkoz√°sk√©ppen. El≈ësz√∂ris, figyelj√ºk meg hogyan √©p√ºl fel
                        megint az adat, amit megakarn√°nk szerezni.
                    </p>
                    <figure>
                        <img class="mx-auto"
                            src="../img/fig5.png"
                            alt="√Åbra 5">
                        <figcaption class="text-center pt-3 pb-4 text-md">5. √Åbra: Nobel-gy≈ëztes nemzetis√©ge
                        </figcaption>
                    </figure>
                    <p class="text-lg font-normal">A k√∂vetkez≈ë f√ºggv√©ny veszi egyes√©vel a gy≈ëzteseket a kor√°bban kapott dict-ekb≈ël
                        √©s visszat√©r√≠t egy n√©v-c√≠mk√©zett dict-et a 'Nationality' kulccsal, ha tal√°lt:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code
                        class="python">def get_winner_nationality(w):
	""" Scrapelj√ºk az √©letrajzi adatokat a gy≈ëzt√©s wikip√©dia oldal√°r√≥l """

	data = requests.get(BASE_URL + w['link'], headers=HEADERS)
	soup = BeautifulSoup(data.content, 'lxml')

	person_data = {'name': w['name']}
	attr_rows = soup.select('table.infobox tr')

	for tr in attr_rows:
		try:
			attribute = tr.select_one('th').text
			if attribute == 'Nationality':
				person_data[attribute] = tr.select_one('td').text
		except AttributeError:
			pass

	return person_data

wdata = []

for w in ws[:50]:
	wdata.append(get_winner_nationality(w))

missing_nationality = []

for w in wdata:
	if not w.get('Nationality'):
		missing_nationality.append(w)

print(missing_nationality)</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Az utols√≥ teszt r√°vil√°g√≠t arra, hogy nem minden esetben volt sikeres. A
                        probl√©ma oka az, hogy nincs egy szabv√°ny √©letrajzi forma, vagy ak√°r szinonim√°kat is haszn√°lhatnak, amely
                        tov√°bbi bonyodalmakat okoz. Alapvet≈ëen, √≠gy azt lehet kik√∂vetkeztetni, hogy gyakran a megfelel≈ë minta
                        megtal√°l√°s√°hoz relat√≠ve sok tesztre √©s pr√≥b√°lkoz√°sra van sz√ºks√©g√ºnk.
                    </p>
                </section>

                <section id="3scrapy">
                    <h2 class="text-2xl pt-8 pb-4 font-semibold text-gray-800"
                        role="button"
                        tabindex="0"
                        aria-pressed="false">A komoly scrapel√©s Scrapy-vel</h2>
                    <p class="text-lg font-normal pb-3">Ahogy egyre √∂sszetetebb scrapel√©si c√©lokat t≈±z√∂l ki magad el√©, √∫gy
                        bonyol√≥dik a folyamat, √©s egy id≈ë ut√°n m√°r sajnos a BeautifulSoup √©s reqeuests p√°ros nem lesz el√©g
                        amb√≠ci√≥id el√©r√©s√©hez. Alapvet≈ëen, a legnagyobb probl√©m√°t a szinkron lek√©rdez√©sekkel van, amikoris a dolgok
                        nagyon k√∂nnyen lelassulhatnak, amennyiben a sz√°muk megsokasodik. Teh√°t, olyan probl√©m√°k jelentkeznek,
                        amikre nem sz√°m√≠tott√°l, √©s ilyenkor j√≥ az, hogy l√©teznek olyan k√∂nyvt√°rak, amelyek sokoldal√∫bbak,
                        robusztusabbak √©s k√©pesek megoldani ezeket a probl√©m√°kat √∂nmagukban, erre j√≥ a Scrapy.</p>
                    <p class="text-lg font-normal pb-3">Mik√∂zben a BeautifulSoup nagyon hasznos kis eszk√∂z gyors √©s 'dirty'
                        scrapekhez, addig a Scrapy az a Python k√∂nyvt√°r, amely a legnagyobb adatokkal is k√∂nny≈±szerrel elb√°nik.
                        Minden dologgal √∫gy mond a dobozb√≥l rendelkezik, amit kor√°bban is haszn√°ltunk: be√©p√≠tett cache, lej√°rati
                        id≈ëvel, aszinkron lek√©rdez√©sek, User-Agent randomiz√°l√°s, √©s m√©g sok egy√©b. Azonban, az eg√©sznek √°ra is
                        van, hiszen a Scrapy-t k√∂zel sem annyira egyszer≈± megtanulni, de a k√∂vetkez≈ë p√©ld√°kban m√©gis megpr√≥b√°ljuk.
                    </p>
                    <p class="text-lg font-normal pb-3">Az el≈ëz≈ë r√©szben sikeresen kinyert√ºnk egy olyan adatsort, amely
                        tartalmazza a Nobel-gy≈ëzteseket n√©vvel, √©vvel √©s kateg√≥ri√°val. Azt√°n megpr√≥b√°ltuk mindegyik nemzetis√©g√©t
                        is lek√©rni, t√∂bb-kevesebb sikerrel. A fejezetben fentebb helyezz√ºk a l√©cet √©s megpr√≥b√°ljuk a fenti adatok
                        mellett olyanokat is megszerezni, mint: orsz√°g, sz√ºlet√©si √©s elhal√°loz√°si d√°tum, nem, sz√ºlet√©si √©s
                        elhal√°loz√°si hely, stb. Legv√©g√©n pedig ak√°r meg is pr√≥b√°lhatjuk megszerezni a gy≈ëztesek k√©peit, ahol ez
                        lehets√©ges, √©s egy kev√©s √©letrajzi adatot is, amelyeket majd k√©s≈ëbb a vizualiz√°ci√≥ sor√°n fogunk
                        felhaszn√°lni:
                    </p>
                    <figure>
                        <img class="mx-auto"
                            src="../img/fig6.png"
                            alt="√Åbra 6">
                        <figcaption class="text-center pt-3 pb-4 text-md">6. √Åbra: Nobel-gy≈ëztes oldal</figcaption>
                    </figure>
                    <p class="text-lg font-normal">Elkezdve, el≈ësz√∂ris t√∂lts√ºk le magunknak a Scrapy-t, amely a
                        szabv√°nyk√∂nyvt√°rnak nem r√©sze, de pip-el k√∂nnyed√©n feltelep√≠thet≈ë:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="shell">$ pip install scrapy</code><button id="copy"
                            class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Miut√°n sikeresen feltelep√≠tett√ºk a Scrapy-t, mostm√°r el√©rhetj√ºk a scrapy
                        parancsot. Ellent√©tben a legt√∂bb Python k√∂nyvt√°rral, a Scrapy-t √∫gy tervezt√©k, hogy a parancssorb√≥l
                        futhasson a scraping projekt kontextus√°n bel√ºl, meghat√°rozva konfigur√°ci√≥s f√°jlokkal, 'scraping spiders',
                        pipeline-okkal, stb. Hozzunk l√©tre egy √∫j projektet a Nobel-d√≠jas scrape√ºnkh√∂z, a startproject opci√≥val.
                        Alapvet≈ëen, ez az opci√≥ nem m√°st tesz, mint l√©trehoz egy projekt mapp√°t, teh√°t el≈ëny√∂s, ha egy erre
                        kialak√≠tott mapp√°ban futtatjuk le:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="shell">$ scrapy startproject nobel_winners
New Scrapy project 'nobel_winners', using template directory 'd:\github\misc\anaconda\lib\site-packages\scrapy\templates\project', created in:
    D:\GitHub\datavisualization\python\nobel_prize\nobel_winners

You can start your first spider with:
    cd nobel_winners
    scrapy genspider example example.com</code><button id="copy"
        class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal pb-3">Teh√°t, ahogy a fenti utas√≠t√°s is hangoztatja, a nobel_winners mapp√°ba kell
                        bel√©pn√ºnk ahhoz, hogy haszn√°lhassuk a scrapy-t. A projekt mapp√°ban bel√ºl megfigyelhet≈ë, hogy eg√©sz sz√©p
                        strukt√∫r√°t inicializ√°lt a scrapy, k√ºl√∂nb√∂z≈ë f√°jlokkal. Emellett, l√©trehozott egy config √°llom√°nyt is, a
                        mapp√°n k√≠v√ºl 'scrapy.cfg' n√©ven. √çgy, a nobel_winners almappa teh√°t tartalmazza a Python m√≥dult, amit
                        __init__.py f√°jl jel√∂l, n√©h√°ny skeleton f√°jllal √©s spiders mapp√°val, amely majd a scrapereket fogja
                        tartalmazni.
                    </p>
                    <p class="text-lg font-normal pb-3">Visszat√©rve m√©gegyszer a kor√°bbi nemzetis√©ges kudarcunkra, teh√°t nem
                        siker√ºlt
                        minden Nobel-gy≈ëztes nemzetis√©g√©t kinyerni az egyszer≈± megold√°ssal, k√∂vetkezetess√©g hi√°ny√°b√≥l legf≈ëk√©ppen.
                        Ugyanakkor, ahelyett, hogy indirekt m√≥don szerezn√©nk meg a nemzetis√©gi adatokat, egy kis Wikip√©dia
                        keresg√©l√©s r√°mutat arra, hogy van m√°s megold√°s is. Van egy oldal, amely felsorolja a gy≈ëzteseket
                        orsz√°gonk√©nt. Ebben az esetben azonban nem t√°bl√°zatos form√°t v√°lasztottak, hanem egy √©v szerinti cs√∂kken≈ë
                        sorrendbe rendezett list√°ban sorolt√°k fel a gy≈ëzteseket. A hagyom√°nyos megk√∂zel√≠t√©ssel neh√©z lenne ebb≈ël
                        haszn√°lhat√≥ adatokat kinyerni, t√∫lzottan rendezetlen√ºl vannak elhelyezve az adatok, teh√°t √∂nmag√°ban az
                        adat nincs megfelel≈ëen megszervezve. Azonban megl√°ssuk majd, hogy n√©h√°ny j√≥l-megszervezett Scrapy lek√©r√©s
                        csod√°kat k√©pes m≈±velni.
                    </p>
                    <p class="text-lg font-normal pb-3">A k√∂vetkez≈ë √°bra azt szeml√©lteti, hogy mi lesz a kezd≈ë oldala az els≈ë
                        spider√ºnknek, egy√ºtt azokkal a kulcs-elemekkel, amiket megc√©lzunk:
                    </p>
                    <figure>
                        <img class="mx-auto"
                            src="../img/fig7.png"
                            alt="√Åbra 7">
                        <figcaption class="text-center pt-3 pb-4 text-md">7. √Åbra: Az els≈ë spider√ºnk kezd≈ë oldala</figcaption>
                    </figure>
                    <p class="text-lg font-normal pb-3">Ahhoz, hogy megszerezz√ºk a sz√ºks√©ges adatot seg√≠ts√©g√ºl h√≠vva megintcsak a
                        Chrome DevTools-t megkeress√ºk azokat a HTML tag-eket, amelyek sz√°munkra relev√°nsak az els≈ë spider√ºnkh√∂z, a
                        fejl√©c c√≠met, ami egy h2 tag, tartalmazza az orsz√°got, majd egy rendezett list√°n, ol-en, bel√ºl a gy≈ëztesek
                        lista elemenk√©nt, li-ben:
                    </p>
                    <figure>
                        <img class="mx-auto"
                            src="../img/fig8.png"
                            alt="√Åbra 8">
                        <figcaption class="text-center pt-3 pb-4 text-md">8. √Åbra: A sz√ºks√©ges HTML tagek</figcaption>
                    </figure>
                    <p class="text-lg font-normal">A Scrapy az lxml-hez hasonl√≥an Xpath-et haszn√°l ahhoz, hogy megc√©lozza a
                        HTML tageket, az xpath m√°r tudjuk, hogy egy szintaxis, amely a HTML dokumentum r√©szeit k√©pes el√©rni, n√©ha
                        meglehet≈ës√©gen bonyolultt√° tud v√°lni, de szerencs√©re a Chrome DevTools-b√≥l m√°r kor√°bban eml√≠tett
                        megold√°ssal k√©pesek vagyunk b√°rmely HTML tag xpath el√©r√©s√©t megszerezni. N√©zz√ºk meg m√©gegyszer, √≠gy n√©z ki
                        egy HTML tag xpath-je:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="plaintext">//*[@id="mw-content-text"]/div/h3[1]

# bontsuk darabokra
# a //*[@id="mw-content-text"] kiv√°laszt egy b√°rmilyen elemet, amely rendelkezik a megadott id-vel
# a /div/h3 kiv√°lasztja az eml√≠tett elemen bel√ºli div child elemet, majd azon bel√ºl a h3 child elemet
# a [1] pedig az els≈ë ilyen elemet jel√∂li, √©rdekess√©g, itt nem 0 kezd√©s≈± indexek vannak, hanem 1-t≈ël kezd≈ëd≈ëek</code><button
    id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">A megfelel≈ë xpath target megtal√°l√°sa kulcsfontoss√°g√∫ a sikeres scrapehez,
                        √©s sok esetben mag√°ba foglal n√©h√°ny pr√≥b√°lkoz√°st. A Scrapy ezt a folyamatot nagyban megk√∂nny√≠ti, hiszen
                        biztos√≠t egy parancssori shell-t, amely bevesz egy URL-t √©s l√©trehoz egy v√°lasz-kontextust (response
                        context), ahol kipr√≥b√°lhatjuk az xpath-jeinket:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="shell">$ scrapy shell https://en.wikipedia.org/wiki/List_of_Nobel_laureates_by_country</code><button id="copy"
                            class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Mostm√°r, hogy megvan a shell-√ºnk, kis√©rletezhet√ºnk. Pr√≥b√°ljuk meg megszerezni
                        az √∂sszes h2 taget az oldalr√≥l:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="">In [2]: h2s = response.xpath('//h2')</code><button id="copy"
                            class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Teh√°t, ebben az esetben kaptunk egy list√°nyi h2 elemet, amely az adott HTML
                        oldalon megtal√°lhat√≥. Most v√°lasszunk ki a list√°b√≥l egyet, mondjuk az els≈ët, √©s a Shell 'code-complete' √©s
                        'syntax-highlighting' adotts√°gait kihaszn√°lva vizsg√°ljuk meg annak met√≥dusait √©s tulajdons√°gait, ehhez
                        el√©gs√©ges be√≠rni az adott v√°ltoz√≥t, majd a . oper√°tor be√≠r√°sa ut√°na egyszer≈±en tab-ot nyomni:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="">In [5]: h2 = h2s[0]

In [6]: h2.2020-07-07 12:00:56 [parso.python.diff] DEBUG: diff parser start
2020-07-07 12:00:56 [parso.python.diff] DEBUG: line_lengths old: 1; new: 1
2020-07-07 12:00:56 [parso.python.diff] DEBUG: -> code[replace] old[1:1] new[1:1]
2020-07-07 12:00:56 [parso.python.diff] DEBUG: parse_part from 1 to 1 (to 0 in part parser)
2020-07-07 12:00:56 [parso.python.diff] DEBUG: diff parser end
In [7]: h2.
            attrib               get()                re()                 remove()             root                 type
            css()                getall()             re_first()           remove_namespaces()  selectorlist_cls     xpath()
            extract()            namespaces           register_namespace() response             text
In [6]: h2.extract()
Out[6]: '&#60;h2 id="mw-toc-heading">Contents&#60;/h2>'</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Gyakran fogjuk haszn√°lni az extract met√≥dust, amely seg√≠ts√©g√©vel el√©rhetj√ºk a
                        'nyers eredm√©ny√©t' egy xpath selectornak. √çgy megmutatja azt, hogy az el√©rt els≈ë h2 tag, val√≥j√°ban a
                        tartalomjegyz√©ket tartalmazza. Ugyanakkor, a val√≥di tartalom ami sz√°munkra √©rdekes h3 tagekben van:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="">In [16]: h3s = response.xpath('//h3')
In [18]: len(h3s)
Out[18]: 88</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Ebb≈ël a megfigyel√©s√ºnkb≈ël azt is k√©pesek vagyunk kik√∂vetkeztetni, hogy a
                        fejl√©cek egy 'mw-headline' classal rendelkeznek, amely nagyban megk√∂nny√≠ti az elemek sz≈±r√©s√©t. Pr√≥b√°ljuk
                        ki egy xpath-en a text met√≥dust, amellyel kinyerhetj√ºk a sz√∂veget egy 'mw-headline' classal rendelkez≈ë
                        span-b≈ël:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="">In [20]: h3_arg = h3s[0]

In [21]: country = h3_arg.xpath('span[@class="mw-headline"]/text()').extract()

In [22]: country
Out[22]: ['Argentina']</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal pb-3">Megfigyelhett√ºk, hogy az extract met√≥dus visszat√©r√≠t egy list√°nyi
                        tal√°latot,
                        ebben az esetben egyetlen stringet tartalmaz√≥ list√°t. Ez √°ltal megtudtuk azt, hogy ha v√©gig iter√°lunk a
                        h3s list√°n, akkor k√©pesek vagyunk az orsz√°gok nev√©t kinyerni.
                    </p>
                    <p class="text-lg font-normal">Felt√©telezve, hogy rendelkez√ºnk az el≈ëz≈ë pontokban kinyert h3 fejl√©cekkel, most
                        valamilyen m√≥don megkellene szerezz√ºk a rendezett list√°kat, amelyek az orsz√°gnevek ut√°n vannak. Erre
                        nagyszer≈±en haszn√°lhat√≥ az xpath <em>following-sibling</em> selectorja. Szerezz√ºk is meg az els≈ë rendezett
                        list√°t Argent√≠na ut√°n:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="">In [23]: ol_arg = h3_arg.xpath('following-sibling::ol[1]')

In [24]: ol_arg
Out[24]: [&#60;Selector xpath='following-sibling::ol[1]' data='&#60;ol>&#60;li>&#60;a
href="/wiki/C%C3%A9sar_Mil...'>]</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">A fenti esetben hi√°ba mondhat√≥ el az, hogy val√≥j√°ban egyetlen Selector van,
                        m√©gis egy SelectorList-et t√©r√≠t vissza az xpath, ezt a probl√©m√°t megoldhatjuk √∫gy, hogy k√∂zvetlen√ºl
                        kiv√°lasztjuk m√°r lek√©rdez√©skor az els≈ë elemet:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="">In [25]: ol_arg = h3_arg.xpath('following-sibling::ol[1]')[0]

In [26]: ol_arg
Out[26]: [&#60;Selector xpath='following-sibling::ol[1]' data='&#60;ol>&#60;li>&#60;a
href="/wiki/C%C3%A9sar_Mil...'>]</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Megvan a list√°nk, mostm√°r meg is szerezhetj√ºk a benne tal√°lhat√≥ lista elemeket:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="">In [27]: lis_arg = ol_arg.xpath('li')

In [28]: lis_arg
Out[28]:
[&#60;Selector xpath='li' data='&#60;li>&#60;a href="/wiki/C%C3%A9sar_Milstei...'>,
 &#60;Selector xpath='li' data='&#60;li>&#60;a href="/wiki/Adolfo_P%C3%A9rez_...'>,
 &#60;Selector xpath='li' data='&#60;li>&#60;a href="/wiki/Luis_Federico_Lelo...'>,
 &#60;Selector xpath='li' data='&#60;li>&#60;a href="/wiki/Bernardo_Houssay" ...'>,
 &#60;Selector xpath='li' data='&#60;li>&#60;a href="/wiki/Carlos_Saavedra_La...'>]</code><button id="copy"
     class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Vizsg√°ljuk meg az egyiket az extract met√≥dussal. Els≈ëk√©nt, megszeretn√©nk
                        szerezni a nyertes nev√©t √©s a lista elemben tal√°lhat√≥ sz√∂veget:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="">In [29]: li = lis_arg[0]

In [30]: li
Out[30]: &#60;Selector xpath='li' data='&#60;li>&#60;a href="/wiki/C%C3%A9sar_Milstei...'>

In [31]: li.extract()
Out[31]: '&#60;li>&60;a href="/wiki/C%C3%A9sar_Milstein" title="C√©sar Milstein">C√©sar Milstein&#60;/a>*, Physiology or Medicine,
1984&#60;/li>'</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">A fenti r√©szb≈ël kider√ºl, hogy a lista elemek egy mint√°t k√∂vetnek szerkezetileg,
                        a n√©v egy anchor tag-en bel√ºl van, a gy≈ëztes Wikip√©dia oldal√°ra hivatkozva, majd vessz≈ëvel elv√°lasztva
                        tal√°lhat√≥ meg a kateg√≥ria, illet≈ëleg az √©v. Egy robusztus megold√°s a gy≈ëztes nev√©nek scrapel√©s√©re, ha
                        kiv√°lasztjuk az eml√≠tett anchor tageket:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="">In [32]: name = li.xpath('a//text()')[0].extract()

In [33]: name
Out[33]: 'C√©sar Milstein'</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Gyakran hasznos lehet, ha k√©pesek vagyunk egy valamilyen elemben tal√°lhat√≥
                        √∂sszes sz√∂veget kinyerni egyidej≈±leg, kihagyva b√°rmilyen HTML tag jel√∂l√©st, csakis a val√≥di tartalmat,
                        erre haszn√°lhat√≥ a <em>descendent-or-self</em>:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="">In [34]: list_text = li.xpath('descendant-or-self::text()').extract()

In [35]: list_text
Out[35]: ['C√©sar Milstein', '*, Physiology or Medicine, 1984']</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal pb-3">Megalkottuk az √∂sszes sz√ºks√©ges xpath-et a scrapel√©si targetjeinkhez, most
                        foglaljuk bele ≈ëket egy Scrapy spiderbe.
                    </p>
                    <p class="text-lg font-normal">Ahogy azt kor√°bban is l√°thattuk, a Scrapy xpath szelekci√≥i visszat√©r√≠tenek egy
                        list√°nyi selectort, amelyek ugyancsak rendelkeznek az eml√≠tett xpath met√≥dussal. Amikor haszn√°ljuk az
                        xpath met√≥dust jobb, ha k√©pben vagyunk a relat√≠v √©s abszol√∫t szelekci√≥k k√∂z√∂tti k√ºl√∂nbs√©gekkel. Vegy√ºk a
                        Nobel-d√≠jas oldalon tal√°lhat√≥ tartalomjegyz√©ket p√©ldak√©ppen, amely a k√∂vetkez≈ë strukt√∫r√°val rendelkezik:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="">&#60;div id='toc'... >
 &#60;ul ... >
    &#60;li ... >
        &#60;a href='Argentina'> ... &#60;/a>
    &#60;/li>
    ...
 &#60;/ul>
&#60;/div></code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Alapvet≈ëen, k√©pesek lenn√©nk el√©rni a tartalomjegyz√©ket a k√∂vetkez≈ë xpath
                        query-vel:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="">In [36]: toc = response.xpath('//div[@id="toc"]')[0]

In [37]: toc
Out[37]: &#60;Selector xpath='//div[@id="toc"]' data='&#60;div id="toc" class="toc" role="navig...'></code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Ha megszeretn√©nk szerezni az √∂sszes orsz√°g &#60li> lista tagj√©t, abban az
                        esetben haszn√°lhatn√°nk egy relat√≠v xpath-et a kiv√°lasztott toc div-en. A k√∂vetkez≈ëk√©ppen:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="">In [46]: lis = toc.xpath('.//ul/li')

In [47]: len(lis)
Out[47]: 80</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Gyakori hiba az, hogy nem-relat√≠v xpath selectort haszn√°lunk a jelenlegi
                        kiv√°laszt√°son, amely a teljes dokumentumb√≥l v√°laszt ki elemeket √©s nem a jelenlegi kontextuson bel√ºl:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="">In [51]: lis = toc.xpath('//ul/li')

In [52]: len(lis)
Out[52]: 240</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal pb-3">Felfegyverkezve a sz√ºks√©ges xpath tud√°ssal, l√©tre is hozhatjuk az els≈ë
                        scraper√ºnket, amelynek c√©lja az lesz, hogy megszerezze nek√ºnk az orsz√°gneveket √©s illet≈ëleg az
                        orsz√°gneveken bel√ºli list√°b√≥l a teljes sz√∂veget.
                    </p>
                    <p class="text-lg font-normal pb-3">A Scrapy a scraperjeit spider-eknek h√≠vja, amelynek mindegyike egy Python
                        modul, amely a spiders mapp√°ban tal√°lhat√≥, amit l√©trehozott a projekt ind√≠t√°sakor. Mi az els≈ë ilyen
                        modulunkat <em>nwinner_list_spider.py</em>-nak fogjuk h√≠vni.
                    </p>
                    <p class="text-lg font-normal">A spiderek val√≥j√°ban subclassok, amelyek a scrapy.Spider classot b≈ëv√≠tik
                        ki, tov√°bb√° b√°rmit helyez√ºnk el a spiders mapp√°ba automatikusan √©rz√©kelve lesz a Scrapy √°ltal √©s
                        el√©rhet≈ëv√© lesz t√©ve a scrapy paranccsal. A k√∂vetkez≈ë p√©lda a leggyakoribb mint√°t fogja bemutatni, amelyre
                        a legt√∂bb spider-edet alapozni fogod:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">import scrapy
import re

class NWinnerItem(scrapy.Item):
    county = scrapy.Field()
    name = scrapy.Field()
    link_text = scrapy.Field()

class NWinnerSpider(scrapy.Spider):
    """ Megszerzi az orsz√°gneveket √©s a link texteket a Nobel-gy≈ëzteseknek """

    name = 'nwinners_list'

    allowed_domains = ['en.wikipedia.org']
    start_urls = ['https://en.wikipedia.org/wiki/List_of_Nobel_laureates_by_country']

    def parse(self, response):
        h3s = response.xpath('//h3')
        # megszerezz√ºk az √∂sszes &#60;h3>-at a weboldalon, a legt√∂bb ilyen az orsz√°gneveket tartalmazza

        for h3 in h3s:
            country = h3.xpath('span[@class="mw-headline"]/text()').extract()
            # ahol lehets√©ges, megszerzi a &#60;h3> elem azon child &#60;span>-j√©t, amely rendelkezik az mw-headline classal

            if country:
                winners = h3.xpath('following-sibling::ol[1]')
                # megszerzi a h3-at k√∂vet≈ë rendezett list√°t, amely a nyerteseket tartalmazza

                for w in winners.xpath('li'):
                    # a list√°n v√©gighaladva megszerzi a &#60;li>-ben tal√°lhat√≥ sz√∂veget

                    text = w.xpath('descendant-or-self::text()').extract()

                    # majd t√©r√≠ti a tal√°latokat, de yield seg√≠ts√©g√©vel, amely lehet≈ës√©get biztos√≠t arra, 
                    # hogy √∫m. sz√ºneteltess√ºk a f√ºggv√©nyt √©s meg≈ërzi √°llapot√°t ahov√° visszat√©rhet
                    # a v√©grehajt√°s
                    yield NWinnerItem(country = country[0], name=text[0], link_text = ' '.join(text))</code><button id="copy"
                        class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal pb-3">A l√©trehozott parse met√≥dus megkapja a Wikip√©dia oldalhoz int√©zett HTTP
                        request
                        v√°lasz√°t, √©s Scrapy Itemeket yield-el, amelyek JSON objektumokk√° lesznek √°talak√≠tva √©s hozz√°f≈±zve az
                        output √°llom√°nyhoz.
                    </p>
                    <p class="text-lg font-normal">Hajtsuk is v√©gre az els≈ë spider-√ºnket, hogy megfigyelj√ºk val√≥ban helyesen
                        m≈±k√∂dik-e √©s megkapjuk-e a k√≠v√°nt adatokat. Megtekinthetj√ºk a rendelkez√©sre √°ll√≥ spider-eket a k√∂vetkez≈ë
                        paranccsal:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="shell">$ scrapy list
nwinners_list</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Ahogy az v√°rhat√≥ volt egyetlen spider-rel rendelkez√ºnk <em>nwinners_list</em>
                        n√©ven. Ahhoz, hogy elkezdj√ºk a scrapel√©st, haszn√°lnunk kell a crawl parancsot √©s megjel√∂lj√ºk az output
                        √°llom√°nyt, amely jelenesetben a nwinners.json lesz:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="shell">$ scrapy crawl nwinners_list -o nwinners.json</code><button id="copy"
                            class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal pb-3">Kellemes esetben egy el√©g sz√©p log-ot is t√°rs√≠t a crawl parancs mell√© a
                        scrapy,
                        amelyben √∫m. besz√°mol mindenr≈ël, amit ≈ë tett, ebben az esetben azt is megmutatja, hogy mit siker√ºlt
                        megszerezni, de legf≈ëk√©ppen azt is, hogy h√°ny Scrapy Itemet siker√ºlt sikeresen megszereznie. V√©g√ºl,
                        megfigyelhet≈ë a crawl eredm√©nye a megadott JSON √°llom√°nyban.
                    </p>
                    <p class="text-lg font-normal pb-3">Miut√°n elmondhatjuk azt, hogy a Scrapy sikeresen lescrapelte a k√≠v√°nt
                        adatokat,
                        az orsz√°gneveket, a gy≈ëztes nev√©t √©s a lista elemben tal√°lhat√≥ minden sz√∂veget, ak√°r kiss√© finom√≠thatunk
                        is rajta, hogy mindazon adatokat is egy√∫ttal megszerezz√ºk, amely a v√©gs≈ë vizualiz√°ci√≥nkhoz kellene.
                    </p>
                    <p class="text-lg font-normal">El≈ësz√∂ris, foglaljuk be mindazon mez≈ëket, amelyre sz√ºks√©g√ºnk lesz jelenesetben
                        a scrapy.Item-be:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">class NWinnerItem(scrapy.Item):
    name = scrapy.Field()
    link = scrapy.Field()
    year = scrapy.Field()
    category = scrapy.Field()
    country = scrapy.Field()
    gender = scrapy.Field()
    born_in = scrapy.Field()
    date_of_birth = scrapy.Field()
    date_of_death = scrapy.Field()
    place_of_birth = scrapy.Field()
    place_of_death = scrapy.Field()
    text = scrapy.Field()</code><button id="copy"
        class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Ugyanakkor, az is j√≥ √∂tlet lenne, ha leegyszer≈±s√≠ten√©nk egy kicsit a k√≥dot, az
                        √°ltal, hogy l√©trehozunk egy <em>process_winner_li</em> nev≈± f√ºggv√©nyt, amely feldolgozza a gy≈ëztes
                        linkj√©nek sz√∂veg√©t. Alapvet≈ëen, egy link selectort √©s egy orsz√°gnevet fogunk √°tadni neki √©s visszat√©r√≠t
                        egy dict-et, amely tartalmazza a k√≠v√°nt adatokat:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">class NWinnerItem(scrapy.Item):
    name = scrapy.Field()
    link = scrapy.Field()
    year = scrapy.Field()
    category = scrapy.Field()
    country = scrapy.Field()
    gender = scrapy.Field()
    born_in = scrapy.Field()
    date_of_birth = scrapy.Field()
    date_of_death = scrapy.Field()
    place_of_birth = scrapy.Field()
    place_of_death = scrapy.Field()
    text = scrapy.Field()

class NWinnerSpider(scrapy.Spider):
    """ Megszerzi a NWinnerItem-ben meghat√°rozott adatokat """

    name = 'nwinners_list'

    allowed_domains = ['en.wikipedia.org']
    start_urls = ['https://en.wikipedia.org/wiki/List_of_Nobel_laureates_by_country']

    def parse(self, response):
        h3s = response.xpath('//h3')

        for h3 in h3s:
            country = h3.xpath('span[@class="mw-headline"]/text()').extract()

            if country:
                winners = h3.xpath('following-sibling::ol[1]')

                for w in winners.xpath('li'):
                    wdata = process_winner_li(w, country[0])

                    yield NWinnerItem(name = wdata['name'], link = wdata['link'], year = wdata['year'], category = wdata['category'], country = wdata['country'], born_in = wdata['born_in'], text = wdata['text'])

BASE_URL = 'http://en.wikipedia.org'

def process_winner_li(w, country=None):
    """ Feldolgozza a gy≈ëztes &#60;li> tagj√©t, hozz√°adva a sz√ºlet√©si orsz√°got vagy nemzetis√©get, ha lehets√©ges """

    wdata = {}

    # az√©rt van sz√ºks√©g az egyenl≈ës√©g jobb oldal√°n tal√°lhat√≥ r√©szre, mert a href a k√∂vetkez≈ë alakot veszi fel a wiki-n: /wiki/...
    # a xpath seg√≠ts√©g√©vel megkeress√ºk azt az anchor taget, amely rendelkezik hivatkoz√°si linkkel
    wdata['link'] = BASE_URL + w.xpath('a/@href').extract()[0]

    # kiveszi a teljes sz√∂veget a &#60;li>-b≈ël, majd a nevet kiv√°lasztja ebb≈ël a sorb√≥l
    text = ' '.join(w.xpath('descendant-or-self::text()').extract())
    wdata['name'] = text.split(',')[0].strip()

    # regExp, amely megtal√°l a stringben egy olyan match-et, amely 4 sz√°mb√≥l √°ll
    year = re.findall('\d{4}', text)

    if year:
        wdata['year'] = int(year[0])
    else:
        wdata['year'] = 0
        print('No year in ', text)

    # regExp, amely kiv√°lasztja a kateg√≥ri√°t
    category = re.findall('Physics|Chemistry|Physiology or Medicine|Literature|Peace|Economics', text)

    if category:
        wdata['category'] = category
    else:
        wdata['category'] = ''
        print('No category in ', text)

    if country:
        # a csillaggal jel√∂lt√©k azt a wiki-n, ha valaki az adott orsz√°gban sz√ºletett, de m√°s nemzetis√©g≈±
        if text.find('*') != -1:
            wdata['country'] = ''
            wdata['born_in'] = country
        else:
            wdata['country'] = country
            wdata['born_in'] = ''

    # lement√ºnk egy m√°solatot a sz√∂vegb≈ël, hogy ak√°r k√©s≈ëbb manu√°lisan is tudjunk az adatokon korrig√°lni
    wdata['text'] = text
    return wdata</code><button id="copy"
        class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal pb-3">K√∂vetkez≈ë l√©p√©sben a <em>wdata</em>-ban visszat√©r√≠tett link mez≈ë lesz a
                        fontos,
                        hiszen azt fogjuk felhaszn√°lni a hi√°nyz√≥ adatok p√≥tl√°s√°ra. A f≈ë Wikip√©dia Nobel-d√≠j gy≈ëztesek list√°ja
                        megadta a legt√∂bb adatot, amelyre sz√ºks√©g√ºnk volt, de m√©g mindig a gy≈ëztesek sz√ºlet√©si √©s elhal√°loz√°si
                        d√°tumjai, valamint a nem√ºk m√©g hi√°nyzik.
                    </p>
                    <p class="text-lg font-normal pb-3">Teh√°t, a k√∂vetkez≈ëkben megvizsg√°ljuk azt, hogy az √©letrajzi Wikip√©dia
                        oldalukr√≥l hogyan lehet kinyerni a k√≠v√°nt √©s hi√°nyos adatokat.
                    </p>
                    <p class="text-lg font-normal pb-3">Kor√°bban a Wikip√©dia rendelkezett egy <em>persondata</em> rejtett
                        t√°bl√°zattal, amely megb√≠zhat√≥ hozz√°f√©r√©st ny√∫jtott olyan adatokhoz, mint p√©ld√°ul sz√ºlet√©si √©s elhal√°loz√°si
                        hely, d√°tumok, stb. Ugyanakkor, ez a hasznos kis forr√°s m√°ra m√°r elavult√° v√°lt √©s nem alkalmazz√°k. A j√≥
                        h√≠r az, hogy ez annak a pr√≥b√°lkoz√°snak a r√©sze, hogy jav√≠ts√°k az √©letrajzi inform√°ci√≥k kategoriz√°lts√°g√°n
                        az √°ltal, hogy ennek dedik√°lt teret hoznak l√©tre a <em>Wikidata</em>-n, a Wikip√©dia k√∂zponti struktur√°lt
                        k√∂zponti adatt√°rol√≥j√°n.
                    </p>
                    <figure>
                        <img class="mx-auto"
                            src="../img/fig9.png"
                            alt="√Åbra 9">
                        <figcaption class="text-center pt-3 pb-4 text-md">9. √Åbra: A rejtett persondata t√°bl√°zat fel√©p√≠t√©se
                        </figcaption>
                    </figure>
                    <p class="text-lg font-normal pb-3">Minden Wikip√©dia oldal tartalmazza a relev√°ns Wikidata oldalt is, az
                        oldals√≥ men√ºbarban.
                    </p>
                    <figure>
                        <img class="mx-auto"
                            src="../img/fig10.png"
                            alt="√Åbra 10">
                        <figcaption class="text-center pt-3 pb-4 text-md">10. √Åbra: A Wikidata el√©r√©se
                        </figcaption>
                    </figure>
                    <p class="text-lg font-normal pb-3">K√∂vetve a fent √°br√°zolt linket az adataink nagy r√©sz√©t megtal√°lhatjuk, a
                        jelent≈ës d√°tumokat √©s helyeket, amelyekre sz√ºks√©g√ºnk van. Alapvet≈ëen, a Wikidata oldal mez≈ëket tartalmaz,
                        minden mez≈ë egy sz√°m√≠t√≥g√©p-gener√°lt HTML, ezekhez kapcsol√≥d√≥ k√≥dokkal, ezek hasznosak lehetnek a scrapel√©s
                        sor√°n.
                    </p>
                    <figure>
                        <img class="mx-auto"
                            src="../img/fig11.png"
                            alt="√Åbra 11">
                        <figcaption class="text-center pt-3 pb-4 text-md">11. √Åbra: A Wikidata fel√©p√≠t√©se
                        </figcaption>
                    </figure>
                    <p class="text-lg font-normal pb-3">Miut√°n az oldalon megkerest√ºk azt a mez≈ët, ami nek√ºnk kell, a Chrome
                        DevTools n√©h√°ny klikk alatt k√©pesek vagyunk annak xpath el√©r√©s√©t megszerezni, hogy a Scrapy-b≈ël k√∂nnyed√©n
                        hivatkozhassunk r√°.
                    </p>
                    <figure>
                        <img class="mx-auto"
                            src="../img/fig12.png"
                            alt="√Åbra 12">
                        <figcaption class="text-center pt-3 pb-4 text-md">12. √Åbra: A Wikidata mez≈ë xpath-j√©nek el√©r√©se
                        </figcaption>
                    </figure>
                    <p class="text-lg font-normal pb-3">Miut√°n minden sz√ºks√©ges el√©r√©st megszerezt√ºnk a keresett adatunkhoz, nincs
                        m√°s h√°tra mind mindezt √∂ssze√°ll√≠tani √©s megvizsg√°lni azt, hogy hogyan kezeli a Scrapy a l√°nc-lek√©r√©seket
                        (chain requests), lehet≈ëv√© t√©ve komplex, t√∂bboldalas scrapel√©si lehet≈ës√©geket.
                    </p>
                    <p class="text-lg font-normal pb-3">Megint csak felmer√ºl az a probl√©ma, hogy ha lehets√©ges, akkor valamilyen
                        m√≥don minimaliz√°ljuk a h√≠v√°sokat √©s lek√©r√©seket a Wikip√©dia oldalhoz, mik√∂zben kis√©rletez√ºnk az
                        xpath-ekkel, teh√°t j√≥ megk√∂zel√≠t√©s, ha t√°roljuk az el√©rt oldalakat. Szerencs√©re, ez a mi eset√ºnkben nem
                        kellene, hogy gondot okozzon, hiszen a Nobel-d√≠jas adatsor mind√∂ssze √©vente egyszer friss√≠t≈ëdik.
                    </p>
                    <p class="text-lg font-normal pb-3">A hagyom√°nyos megk√∂zel√≠t√©s eset√©ben is foglalkoztunk m√°r cache-el, akkor a
                        requests pluginj√°val oldottuk meg, most a Scrapy kifinomult cachel√©si rendszer√©vel fogunk dolgozni, amely
                        seg√≠ts√©g√©vel finomhangolva √©s r√©szletekre bontva vagyunk k√©pesek pontosan meghat√°rozni azt, hogy hogyan
                        szeretn√©k cachelni, p√©ld√°ul adatb√°zis/f√°jlrendszeri t√°rol√°si lehet≈ës√©gek-back-end-ek k√∂z√∂tti v√°laszt√°st
                        tesz lehet≈ëv√©, stb.
                    </p>
                    <p class="text-lg font-normal">Ugyanakkor, miel≈ëtt haszn√°lhatn√°nk azt a projekt√ºnk <em>settings.py</em>
                        m√≥dul√°ban enged√©lyezni kell azt. Sz√°mos be√°ll√≠t√°si lehet≈ës√©g √°ll rendelkez√©s√ºnkre, de jelenesetben az
                        egyszer≈± p√©ld√°nkban el√©gs√©ges lesz a <em>HTTPCACHE_ENABLED</em> igazra √°ll√≠tani:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="plaintext"># Enable and configure HTTP caching (disabled by default)
# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings
HTTPCACHE_ENABLED = True
#HTTPCACHE_EXPIRATION_SECS = 0
#HTTPCACHE_DIR = 'httpcache'
#HTTPCACHE_IGNORE_HTTP_CODES = []
#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal pb-3">Nem ez az egyetlen middleware, amivel a Scrapy rendelkezik, a teljes lista
                        itt
                        megtekinthet≈ë: <a
                            href="https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#downloader-middleware"
                            class="group text-lg pb-8 text-gray-600
                    hover:text-blue-800 opacity-75 hover:opacity-100 transition duration-100 ease-in">link <span class="inline-block font-normal transform group-hover:translate-x-1 transition duration-100
                    ease-in">‚Üó</span></a> .
                    </p>
                    <p class="text-lg font-normal pb-3">A jelenlegi parser met√≥dusunk, ami a Spider√ºnkben tal√°lhat√≥ felhaszn√°lja
                        az
                        √°ltalunk √≠rt process_winners_li f√ºggv√©nyt, hogy megszerezze az orsz√°gadatokat, nevet, √©vet, kateg√≥ri√°t, √©s
                        az √©letrajzi hyperlinket. Nam√°rmost, jelenleg az ut√≥bb eml√≠tett hyperlinket szeretn√©nk felhaszn√°lni arra,
                        hogy Scrapy lek√©rdez√©seket gener√°ljunk, amely el√©ri az √©letrajzi oldalt, √©s ezt az el√©r√©st fogja √°tadni
                        egy met√≥dusnak scrapel√©shez.
                    </p>
                    <p class="text-lg font-normal">A Scrapy Python-ra hajaz√≥ l√°ncol√°si mint√°t tesz lehet≈ëv√©, felhaszn√°lva a Python
                        yield utas√≠t√°s√°t, hogy gener√°torokat hozzon l√©tre, lehet≈ëv√© t√©ve a Scrapy-nek, hogy k√∂nnyen fogyasszon
                        b√°rmilyen tov√°bbi oldal lek√©rdez√©st, amit csak csin√°lunk:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">class NWinnerSpider(scrapy.Spider):
    name = 'nwinners_full'
    allowed_domains = ['en.wikipedia.org']
    start_urls = ['https://en.wikipedia.org/wiki/List_of_Nobel_laureates_by_country']

    def parse(self, response):
        filename = response.url.split('/')[-1]

        # lek√©ri az √∂sszes h3 taget az oldalr√≥l
        h3s = response.xpath('//h3')

        # jelenleg csak az els≈ë 3 orsz√°got fogjuk megszerezni
        for h3 in list(h3s)[:2]:
            # megszerezz√ºk az orsz√°got, √©s ki is bontjuk, megszerezve a teljes tartalm√°t
            country = h3.xpath('span[@class="mw-headline"]/text()').extract()

            # ha az orsz√°gnak nincs valid tartalma, ne csin√°ljon semmit
            if country:
                # megszerezz√ºk a nyertesek list√°j√°t
                winners = h3.xpath('following-sibling::ol[1]')

                # a lista elemein v√©gighaladva megszerezz√ºk az adatokat
                for w in winners.xpath('li'):
                    # feldolgozzuk a lista elemet megszerezve az alapvet≈ë adatokat, amelyek a Nobel-d√≠jas oldalr√≥l kinyerhet≈ëk
                    wdata = process_winner_li(w, country[0])

                    # a tov√°bbi, hi√°nyz√≥ adatokat request l√°ncol√°ssal oldjuk meg
                    # a request funkci√≥ egy request-et csin√°l a gy≈ëztes Wikip√©dia oldal√°ra, felhaszn√°lva a wdata-ban tal√°lhat√≥ hyperlinket
                    # a callback f√ºggv√©nyt √°ll√≠tsuk be a m√°sodik argumentumban, amely az els≈ë argumentumra c√≠mezett request v√°lasz√°t fogja megkapni majd feldolgozni
                    request = scrapy.Request(wdata['link'], callback=self.parse_bio, dont_filter=True)

                    # l√©trehozunk egy Scrapy Itemet, amely tartalmazni fogja a Nobel adatunkat
                    # azt√°n ez az Item adat hozz√°rendel≈ëdik a request metaadatj√°hoz, hogy b√°rmif√©le v√°lasz hozz√°f√©r√©st biztos√≠tson
                    request.meta['item'] = NWinnerItem(**wdata)

                    # az √°ltal, hogy yield-elj√ºk a request-et, a parse met√≥dus egy gener√°torr√° v√°lik, amely felhaszn√°lhat√≥ request-eket t√©r√≠t
                    yield request</code><button id="copy"
                        class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal pb-3">Kor√°bban teh√°t megfigyelt√ºk azt, hogy az √©letrajzi oldalon a Wikidata
                        r√©szt
                        kell megtal√°lnunk, hiszen abban az adatt√°rol√≥ban minden sz√°munkra relev√°ns inform√°ci√≥ megtal√°lhat√≥. Teh√°t,
                        megkell tal√°lnunk a linket √©s egy lek√©rdez√©st ir√°ny√≠tani r√°. Azt√°n m√°r ut√°na meg is szerezhetj√ºk a hi√°nyz√≥
                        nemadatokat, helyeket √©s d√°tumokat.
                    </p>
                    <p class="text-lg font-normal">Erre k√©t k√ºl√∂n met√≥dust fogunk √≠rni, egy <em>parse_bio</em>-t, √©s egy
                        <em>parse_wikidata</em>-t, az ut√≥bbi a Wikidata linkj√©nek megtal√°l√°shoz fogjuk haszn√°lni, az el≈ëbbit pedig
                        a lek√©rdez√©s-l√°ncolat v√©g√©n a hi√°nyz√≥ mez≈ëk bet√∂lt√©s√©re:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">    # ez a met√≥dus fogja kezelni a √©letrajzi oldalra c√≠mzett lek√©rdez call-backj√©t
    def parse_bio(self, response):
        # ahhoz, hogy hozz√°adjuk a scrapelt adatunkat a Scrapy Itemhez, el≈ësz√∂r lek√©rj√ºk azt a response metaadatj√°b√≥l
        item = response.meta['item']

        # megkapja a linket az √©letrajzi oldal Wikidata oldal√°hoz
        href = response.xpath('//li[@id="t-wikibase"]/a/@href').extract()

        if href:
            # felhaszn√°lja a Wikidata linket ahhoz, hogy gener√°ljon egy lek√©rdez√©st a Spider-√ºnk parse_wikidata met√≥dus√°val, mint callback f√ºggv√©ny
            request = scrapy.Request(href[0], callback=self.parse_wikidata, dont_filter=True)

            request.meta['item'] = item

            yield request

    def parse_wikidata(self, response):
        item = response.meta['item']

        # a relev√°ns adatok tartalmaz√≥ k√≥dok, amik a Wikidata forr√°s√°b√≥l vannak, a nev√ºk megegyezik a Scrapy Item√ºkben meghat√°rozott nevekkel,
        # azok amelyek link attrib√∫tummal is rendelkeznek linkk√©nt vannak meghat√°rozva a wikin
        property_codes = [
            {'name':'date_of_birth', 'code':'P569'},
            {'name':'date_of_death', 'code':'P570'},
            {'name':'place_of_birth', 'code':'P19',
            'link':True},
            {'name':'place_of_death', 'code':'P20',
            'link':True},
            {'name':'gender', 'code':'P21', 'link':True}
        ]

        # ez a sz√©ps√©g a Chrome DevTools-b√≥l val√≥
        p_template = '//*[@id="{code}"]/div[2]/div[1]/div/div[2]/div[2]/div[1]{link_html}/text()'

        for prop in property_codes:
            link_html = ''

            if prop.get('link'):
                link_html = '/a'

            # kieg√©sz√≠tj√ºk a fenti template selectorunkat a hi√°nyos r√©szekkel, a format f√ºggv√©nyt haszn√°lva
            # hozz√°adva a sz√ºks√©g /a-t amennyiben linkbe van √°gyazva a Wikin a mez≈ë
            sel = response.xpath(p_template.format(code=prop['code'], link_html=link_html))

            if sel:
                item[prop['name']] = sel[0].extract()

        # v√©g√ºl yieldelj√ºk az item-et, amely ezen a ponton m√°r minden adatot tartalmaznia kellene a wiki-r≈ël
        yield item</code><button id="copy"
            class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Azt√°n n√©zz√ºk meg megfelel≈ëen m≈±k√∂dik-e a scraper, amit √≠rtunk:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="shell">$ scrapy crawl nwinners_full
# Obey robots.txt rules
# ROBOTSTXT_OBEY = False
# a fenti settings.py m√≥dos√≠t√°st eszk√∂z√∂lni kellett, m√°sk√©pp nem m≈±k√∂dik a scrap, a crawl el≈ësz√∂r let√∂lti ezt a robots.txt-t, miel≈ëtt megt√∂rt√©nne a scrape
# ebben az esetben ez megakad√°lyozza a lefut√°st</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal pb-3">A log f√°jlb√≥l kider√ºl, hogy minden j√≥l futott le. Mostm√°r t√©nyleg minden
                        adat rendelkez√©s√ºnkre √°ll, ami alapvet≈ë adatot illet. Ugyanakkor, az alapvet≈ë adatokon t√∫l szeretn√©nk ak√°r
                        k√©peket √©s √©letrajzi t√∂rt√©neteket is, ahol az lehets√©ges.
                    </p>
                    <p class="text-lg font-normal pb-3">Mind eml√≠tett√ºk kis szem√©lyis√©get szeretn√©nk hozz√°adni a v√©gs≈ë
                        projekt√ºnkh√∂z
                        √©letrajzi t√∂rt√©netekkel √©s k√©pekkel. Szerencs√©re a Wikip√©dia √©letrajzi oldalak ezeket √°ltal√°ban
                        tartalmazz√°k, teh√°t j√≥ kiindul√°s.
                    </p>
                    <p class="text-lg font-normal pb-3">Az eddig scrapelt adataink mind stringek voltak, ahhoz, hogy k√©peket
                        szerezz√ºnk meg k√ºl√∂nb√∂z≈ë form√°tumokban, haszn√°lnunk kell a Scrapy <em>pipeline</em>-t. √Åltal√°noss√°gban, a
                        pipeline-ok lehet≈ëv√© teszik, hogy ut√≥feldolgozzuk a scrapelt elemeket √©s l√©nyeg√©ben ak√°rh√°nyat
                        l√©trehozhatsz bel≈ël√ºk. K√©pes vagy saj√°tot is √≠rni vagy felhaszn√°lhatod azokat, amelyeket a Scrapy m√°r
                        alap√©rtelmezetten rendelkez√©sedre bocs√°jt, mint p√©ld√°ul az <em>ImagesPipeline</em> amit most is fogunk
                        haszn√°lni.
                    </p>
                    <p class="text-lg font-normal">A legegyszer≈±bb form√°j√°ban egy pipeline-n√°l csakis egy
                        <em>process_item</em> met√≥dust kell meghat√°rozni. Ez fogja megkapni a scrapelt elemeket √©s a Spider
                        objektumot. A k√∂vetkez≈ëkben l√©trehozunk egy olyan pipeline-t, amely elveti azokat a gy≈ëzteseket a
                        list√°b√≥l, amelyek egyes√ºletek, mintsem szem√©lyek, √≠gy ler√∂vid√≠tve a list√°t, kihagyva a felesleges
                        keres√©seket. Ehhez a <em>pipelines.py</em> m√≥dulba a k√∂vetkez≈ët √≠rjuk:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">from scrapy.exceptions import DropItem

class DropNonPerson(object):
    """ T√∂r√∂lje a nem-egy√©n gy≈ëzteseket """

    def process_item(self, item, spider):
        if not item['gender']:
            # abban az esetben, ha a scrapelt elem√ºnkben nem tal√°lhat√≥ gender, abban az esetben egy egyes√ºletr≈ël besz√©lhet√ºnk
            # mivel a v√©gs≈ë vizualiz√°ci√≥nk a szem√©lyekre √∂sszpontos√≠t, ez√©rt a DropItem-el elvetj√ºk az elemet az output streamb≈ël
            raise DropItem('A k√∂vetkez≈ë gy≈ëztes nem szem√©ly: {}'.format(item['name']))

        # musz√°j visszat√©r√≠ten√ºnk az elemet a met√≥dusb√≥l, tov√°bbi pipeline-ok miatt vagy ak√°r az√©rt, mert a Scrapy-nek lekell mentenie
        return item</code><button id="copy"
            class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">V√©g√ºl, ahhoz, hogy hozz√°adjuk a fent meg√≠rt pipeline-t a projekt√ºnk
                        spider-jeihez, bekell azt jegyezn√ºnk a settings.py-ban, a k√∂vetkez≈ëk√©ppen:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">ITEM_PIPELINES = {
    'nobel_winners.pipelines.DropNonPerson': 1, # az 1-essel akt√≠v√°ljuk a pipeline-t
}</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal pb-3">Miut√°n megismerkedt√ºnk a pipeline-okkal, l√©trehozhatjuk azokat, amelyekkel
                        t√©nyleges dolgozni fogunk. A probl√©m√°nk megold√°s√°hoz el≈ësz√∂ris, az √©letrajzi sz√∂veget a m√°r megszokott
                        m√≥don megszerezhetj√ºk, de a k√©pekkel jobb, ha az ImagePipeline foglalkozik.
                    </p>
                    <p class="text-lg font-normal pb-3">Ak√°r hozz√°adhatn√°nk a fent eml√≠tett scrapeket a m√°r l√©tez≈ë nwinners_full
                        spider√ºnkh√∂z, de az m√°r kezd egy kicsit zs√∫foltt√° v√°lni. Alapvet≈ëen, logikailag sem szembe√ºtk√∂z≈ë az, hogy
                        a klasszikus form√°lis kateg√≥ri√°kt√≥l, a szem√©lyes adatokat sz√©tv√°lasszuk, teh√°t l√©trefogunk hozni egy √∫j
                        spider-t <em>nwinners_minibio</em> n√©ven.
                    </p>
                    <p class="text-lg font-normal pb-3">Ahogy megszokhattuk, az els≈ë dolgunk mindig az, hogy megkeress√ºk azokat a
                        c√©l elemeket, amelyekben az √°ltalunk k√≠v√°nt adatok megtal√°lhat√≥ak, teh√°t keress√ºk azt a taget, amely
                        tartalmazza az √©letrajzi sz√∂vegeket √©s a k√©pet.
                    </p>
                    <figure>
                        <img class="mx-auto"
                            src="../img/fig13.png"
                            alt="√Åbra 13">
                        <figcaption class="text-center pt-3 pb-4 text-md">13. √Åbra: Az √©letrajzi oldalak fel√©p√≠t√©se
                        </figcaption>
                    </figure>
                    <p class="text-lg font-normal pb-3">Megvizsg√°lva az oldal HTML fel√©p√≠t√©s√©t elmondhatjuk azt, hogy a sz√∂veges
                        tartalom egy div tag-en bel√ºl tal√°lhat√≥, amelynek az id-ja az 'mw-content-text', √©s a k√∂vetkez≈ë xpath-el
                        √©rhetj√ºk el: '//*[@id="mw-content-text"]'. Most gondot okozhat az, hogy hol kellene meg√°llnunk a
                        scrapel√©ssel. Kor√°bban, a Wikip√©dia oldalakon azzal jel√∂lt√©k a bevezet≈ë v√©g√©t, hogy egy √ºres paragrafust
                        helyeztek el ott, ahogy azt a fenti √°bra is bemutatja. Manaps√°g ez m√°r nem √≠gy van, kicsit bonyol√≥dik a
                        helyzet. Alapvet≈ëen, most sem t√∫l
                        bonyol√∫lt megtal√°lni a meg√°ll√°si pontot, a bevezet≈ë a tartalomjegyz√©k div el≈ëtt √©r v√©get, teh√°t
                        haszn√°lhatjuk ak√°r azt is.
                    </p>
                    <p class="text-lg font-normal pb-3">R√°t√©rve a k√©pre, megfigyelhetj√ºk azt, hogy a k√©p egy t√°bl√°zatban van,
                        amelynek
                        a k√∂vetkez≈ë xpath el√©r√©se l√©tezik: '//*[@id="mw-content-text"]/div/table[1]'. Szerencs√©nkre, az egyed√ºli
                        img tag, ami ezen a t√°bl√°zaton bel√ºl van az √°ltalunk keresett k√©p a gy≈ëztesr≈ël. √çgy a k√©p hivatkoz√°s√°nak
                        el√©r√©s√©nek
                        xpath-je: '//*[@id="mw-content-text"]/div/table[1]/tbody/tr[2]/td/a/img/@src'.
                    </p>
                    <p class="text-lg font-normal">Ahogy azt kor√°bban is tett√ºk az els≈ë spider√ºnkn√©l, a folyamatot azzal
                        kezdj√ºk, hogy l√©trehozzuk a Scrapy Itemet, amely az adatokat fogja t√°rolni. El≈ësz√∂ris, lescrapelj√ºk a
                        bio-linket, √©s a nev√©t a gy≈ëztesnek, majd ezeket haszn√°lhatjuk azononos√≠t√≥kk√©nt a k√©phez √©s a sz√∂veghez.
                        Sz√ºks√©g√ºnk van egy tov√°bbi v√°ltoz√≥ra is, hogy t√°roljuk az k√©p-linkeket (ebben az esetben csak egy k√©p
                        lesz, de jobb ezzel is megismerkedni), az eredm√©ny√ºl kapott k√©p-hivatkoz√°sokat, v√©g√ºl egy bio_image mez≈ëre
                        is, hogy t√°rolja az √©rdekelt k√©pet:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">import scrapy
import re

BASE_URL = 'http://en.wikipedia.org'

class NWinnerItemBio(scrapy.Item):
    link = scrapy.Field()
    name = scrapy.Field()
    mini_bio = scrapy.Field()
    image_urls = scrapy.Field()
    bio_image = scrapy.Field()
    images = scrapy.Field()

class NWinnerSpiderBio(scrapy.Spider):
    name = 'nwinners_minibio'
    allowed_domains = ['en.wikipedia.org']
    start_urls = ['https://en.wikipedia.org/wiki/List_of_Nobel_laureates_by_country']

    def parse(self, response):
        filename = response.url.split('/')[-1]
        h3s = response.xpath('//h3')

        for h3 in h3s:
            country = h3.xpath('//span[@class="mw-headline"]/text()').extract()

            if country:
                winners = h3.xpath('following-sibling::ol[1]')

                for w in winners.xpath('li'):
                    wdata = {}

                    wdata['link'] = BASE_URL + w.xpath('a/@href').extract()[0]

                    request = scrapy.Request(wdata['link'], callback=self.get_mini_bio)

                    request.meta['item'] = NWinnerItemBio(**wdata)

                    yield request

    def get_mini_bio(self, response):
        """ Lek√©ri a gy≈ëztes √©letrajzi-sz√∂veg√©t √©s k√©p√©t """

        BASE_URL_ESCAPED = 'http:\/\/en.wikipedia.org'

        item = response.meta['item']

        item['image_urls'] = []

        img_src = response.xpath('//*[@id="mw-content-text"]/div/table[1]/tbody/tr[2]/td/a/img/@src')

        if img_src:
            item['image_urls'] = ['https:' + img_src[0].extract()]

        mini_bio = ''
        # elt√©r√ºnk a k√∂nyv xpath-j√©t≈ël, hiszen a mi eset√ºnkben nincs az az √ºres para, amivel ≈ë √°ll meg
        # nek√ºnk a content-text-en bel√ºli √∂sszes elem kell, hogy megkeress√ºk a meg√°ll√°st jelent≈ë tartalomjegyz√©ket
        tags = response.xpath('//*[@id="mw-content-text"]/div/*').extract()

        for t in tags:
            # ahhoz, hogy megkapjuk a keresett jegyz√©ket, regExp-et haszn√°lunk
            if re.findall('id="toc"', t):
                break

            # musz√°j volt belerakjam a korl√°toz√°st a stringbe, m√°sk√©pp n√©ha megbolondult
            if not re.findall('&#60;p>', t[:16]):
                continue

            # mivel a mi eset√ºnkben nyersnek vannak meg a tag-ek, jobb, ha a paragrafus tageket elt√°vol√≠tjuk
            tmp = re.sub('&#60;p>|&#60;/p>', '', t)
            mini_bio += tmp

        mini_bio = mini_bio.replace('href="/wiki', 'href="' + BASE_URL + '/wiki')
        mini_bio = mini_bio.replace('href="#', item['link'] + '#')

        item['mini_bio'] = mini_bio

        yield item
</code><button id="copy"
    class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal pb-3">Miut√°n sikeresen meg√≠rtuk az √∫jabb spider-√ºnket, l√©trekell hozzuk a
                        kieg√©sz√≠t≈ë pipeline-j√°t, amely √°talak√≠tja a k√©p URL-ket √©s lementi, mint k√©p, ehhez fogjuk a m√°r t√∂bbsz√∂r
                        is eml√≠tett ImagesPipeline-t haszn√°lni.
                    </p>
                    <p class="text-lg font-normal">Az ImagesPipeline k√©t f≈ë met√≥dussal rendelkezik, egy
                        <em>get_media_requests</em>-el, amely lek√©rdez√©seket gener√°l a k√©p URL-kre, √©s egy
                        <em>item_completed</em>-el, amelyet az ut√°n h√≠v le miut√°n a lekerdez√©s megt√∂rt√©nt:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">from scrapy.pipelines.images import ImagesPipeline

class NobelImagesPipeline(ImagesPipeline):
    # bevesz b√°rmilyen k√©p URL, amit az nwinners_minibio spider scrapelt √©s gener√°l egy HTTP lek√©rdez√©st a tartalm√°ra
    def get_media_requests(self, item, info):
        for image_url in item['image_urls']:
            yield scrapy.Request(image_url, meta={'dont_redirect': False})

    # miut√°n legener√°l√≥dott a lek√©rdez√©s, ennek eredm√©nye ebbe a met√≥dusba ker√ºl
    def item_completed(self, results, item, info):
        # Mivel, az eredm√©ny tuple-kbe van rendezve a k√∂vetkez≈ë form√°ban: [(True, Image), (False, Image)]
        # kisz≈±rj√ºk azokat az eseteket, amikor nem siker√ºlt a k√©pet lek√©rni
        image_paths = [x['path'] for ok, x in results if ok]

        # azokn√°l, amelyekn√©l siker√ºlt, lementi a el√©r√©s√ºket, ahhoz a mapp√°hoz viszony√≠tva, amelyet
        # a settings.py-ban hat√°roztunk meg:
        # IMAGES_STORE = 'images'
        # ITEM_PIPELINES = {
        # 'nobel_winners.pipelines.DropNonPerson': 1, # az 1-essel akt√≠v√°ljuk a pipeline-t
        # 'nobel_winners.pipelines.NobelImagesPipeline': 1,
        # }
        if image_paths:
            item['bio_image'] = image_paths[0]

        return item</code><button id="copy"
            class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal pb-3">Nincs m√°s dolgunk, mint lefuttatni a l√©trehozott √∫j spider-√ºnket, √©s
                        megvizsg√°lni annak output-j√°t.
                    </p>
                    <p class="text-lg font-normal pb-3">A f√°jlok k√∂z√∂tt megfigyelhet≈ë, hogy minden siker√ºlt, megvannak az
                        √©letrajzi bekezd√©sek √©s a k√©pek f√°jlai is, ahol az lehets√©ges √©s el√©rhet≈ë volt. √ârdekess√©g, hogy a
                        k√©pf√°jlok elnevez√©se egy SHA1 hash az URL-nek, ez √°ltal az image pipeline k√©pes leellen≈ër√≠zni a l√©tez≈ë
                        k√©peket √©s elker√ºlni a felesleges lek√©rdez√©seket m√°r l√©tez≈ë k√©pekhez.
                    </p>
                    <p class="text-lg font-normal pb-3">Azonban, miel≈ëtt v√©gezn√©nk ezzel a fejezettel m√©g besz√©ln√ºnk kell arr√≥l az
                        esetr≈ël is, amikor t√∂bb pipeline-unk van, hiszen a kor√°bbi p√©ld√°k eset√©ben is kett≈ët hoztunk l√©tre, egyet
                        a egyes√ºletek eldob√°s√°ra, a m√°sodikat pedig a k√©pekhez. √Åltal√°noss√°gban, a pipeline-ok, amelyeket a
                        be√°ll√≠t√°sok m√≥dulban enged√©lyez√ºnk az √∂sszes spider-re vonatkoznak, ami l√©tezik a projekt√ºnkben. Ilyen
                        esetekben, lehet az lenne c√©lszer≈±, ha megtudn√°nk hat√°rozni azt, hogy melyik pipeline melyik spider-re
                        vonatkozik. Sz√°mos megk√∂zel√≠t√©s van a probl√©ma megold√°s√°ra.
                    </p>
                    <p class="text-lg font-normal">Az egyik legokosabb d√∂nt√©s az lenne, ha nem √°ltal√°nosan a m√≥dulban, hanem
                        a spider <em>custom_settings</em> tulajdons√°g√°t felhaszn√°lva √°ll√≠tan√°nk be a pipeline-t. A mi eset√ºnkben
                        √≠gy az nwinners_minibio spider a k√∂vetkez≈ëk√©ppen alakulna √°t:
                    </p>
                    <pre class="my-4 text-sm md:text-md font-semibold rounded-md overflow-auto text-gray-200"><code class="python">class NWinnerSpiderBio(scrapy.Spider):
    name = 'nwinners_minibio'
    allowed_domains = ['en.wikipedia.org']
    start_urls = ['https://en.wikipedia.org/wiki/List_of_Nobel_laureates_by_country']

    custom_settings = {
        'ITEM_PIPELINES': {'nobel_winners.pipelines.NobelImagesPipeline': 1,}
    }

    ...</code><button id="copy"
        class="text-sm md:text-md font-semibold"><img src="../img/copy.svg" alt="M√°sol√°s gomb"></button></pre>
                    <p class="text-lg font-normal">Teh√°t, ebben az esetben a NobelImagesPipeline csakis erre a spider-re
                        vonatkozna, a t√∂bbiben nem lenne el√©rhet≈ë scrapel√©s k√∂zben.
                    </p>
                </section>
            </section>

            <!-- harmadik r√©sze v√©ge -->

            <div class="my-8 flex content-center">
                <button
                    class="flex-auto w-24 bg-gray-900 text-white group hover:bg-blue-800 transition duration-75 ease-in"><a
                        class="block px-4 py-2"
                        href="2.html"><span class="inline-block font-normal transform group-hover:-translate-x-1 transition
                            duration-100 ease-in">&larr;</span>
                        El≈ëz≈ë</a></button>
                <button class="flex-auto w-24 ml-4 bg-gray-900 text-white group hover:bg-blue-800 transition duration-75
                    ease-in"><a class="block px-4 py-2"
                        href="4.html">K√∂vetkez≈ë
                        <span
                            class="inline-block font-normal transform group-hover:translate-x-1 transition duration-100 ease-in">&rarr;</span></a></button>
            </div>
        </main>

        <div id="arr"
            class="fixed block transition duration-100 ease-in"><a class="inline-block text-3xl py-3 px-6 hover:text-blue-800 transition duration-100 ease-in transform hover:scale-125
                opacity-50 hover:opacity-100"
                href="#menu">&uarr;</a></div>

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/underscore.js/1.10.2/underscore-min.js"></script>
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
        <script src="../tools/pushy-1.3.0/js/pushy.min.js"></script>
        <script src="../javascript/main.js"></script>
        <script src="../javascript/introd3.js"></script>
    </body>

</html>